# app_v4_with_crawl_stats.py
#
# ============================================================
#  News Keyword Visualizer V4
# ------------------------------------------------------------
#  âœ… V3 ê¸°ëŠ¥ì€ ìœ ì§€í•˜ë©´ì„œ UIë§Œ ê³ ë„í™”í•œ ë²„ì „
#     (UI Improved + Safe Guards)
#
#  + [ì¶”ê°€ ê°œì„ ]
#    âœ… ë°°í¬ í™˜ê²½ì—ì„œ í¬ë¡¤ë§ ì„±ê³µë¥ (ì„±ê³µ/ì‹¤íŒ¨/ì‹¤íŒ¨ì›ì¸)ì„ UIì— í‘œì‹œ
#       - ì‹¤íŒ¨ ì›ì¸ ë¶„ë¥˜:
#         * timeout / connection / request_exception
#         * http_403 / http_429 / http_other
#         * no_selector(#dic_area ì—†ìŒ)
#         * too_short(ì •ì œ í›„ ë„ˆë¬´ ì§§ìŒ)
#         * not_naver(ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ì•„ë‹˜)
#
#  ------------------------------------------------------------
#  ì‹¤í–‰:
#     streamlit run app_v4_with_crawl_stats.py
# ============================================================

import json
import re
import pickle
import html
from datetime import datetime
from email.utils import parsedate_to_datetime
from io import BytesIO
from urllib.parse import quote
import zipfile
from collections import Counter

import requests as rq
import bs4
import pandas as pd
import numpy as np

import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from PIL import Image
from wordcloud import WordCloud
from streamlit_lottie import st_lottie

from sklearn.feature_extraction.text import TfidfVectorizer
from soynlp.noun import LRNounExtractor_v2


# ============================================================
# 0) ì „ì—­ ì„¤ì •(ê²½ë¡œ/íŒŒì¼)
# ============================================================
FONT_PATH = "./resources/NanumSquareR.ttf"
STOPWORDS_PATH = "./resources/stopwords_ko.txt"
TOKENIZER_PATH = "./resources/my_tokenizer1.model"
LOTTIE_PATH = "./resources/lottie-full-movie-experience-including-music-news-video-weather-and-lots-of-entertainment.json"

MASK_BG = {
    "ì—†ìŒ": "./resources/background_0.png",
    "íƒ€ì›": "./resources/background_1.png",
    "ë§í’ì„ ": "./resources/background_2.png",
    "í•˜íŠ¸": "./resources/background_3.png",
}

# í¬ë¡¤ë§ íŒë‹¨ì„ ìœ„í•œ ì„ê³„ê°’
MIN_BODY_LEN = 100  # clean_text_keep_korean ì´í›„ ìµœì†Œ ê¸¸ì´
CRAWL_TIMEOUT = 10  # requests timeout(ì´ˆ)


# ============================================================
# 1) í…Œë§ˆ ì¹œí™” CSS (ë¼ì´íŠ¸/ë‹¤í¬ ê³µìš©)
# ============================================================
def inject_theme_friendly_css() -> None:
    st.markdown(
        """
        <style>
        .nk-title-wrap{
            text-align:center;
            margin: 0.25rem 0 1.0rem 0;
        }
        .nk-title-main{
            font-size: 2.0rem;
            font-weight: 850;
            line-height: 1.1;
            margin: 0;
        }
        .nk-title-sub{
            font-size: 1.1rem;
            font-weight: 700;
            opacity: 0.8;
            margin-top: 0.35rem;
        }

        .nk-card{
            border: 1px solid rgba(128,128,128,0.25);
            border-radius: 14px;
            padding: 12px 14px;
            margin: 10px 0;
            background: rgba(128,128,128,0.06);
        }
        @media (prefers-color-scheme: dark) {
          .nk-card{
            background: rgba(255,255,255,0.04);
            border: 1px solid rgba(255,255,255,0.18);
          }
        }

        .nk-badge{
            display:inline-block;
            padding:6px 12px;
            margin:4px 6px 4px 0;
            border-radius: 16px;
            font-size: 0.92rem;
            font-weight: 750;
            border: 1px solid rgba(128,128,128,0.25);
            background: rgba(99,102,241,0.10);
        }
        @media (prefers-color-scheme: dark) {
          .nk-badge{
            background: rgba(99,102,241,0.18);
            border: 1px solid rgba(255,255,255,0.18);
          }
        }

        mark{
            padding: 0.08em 0.18em;
            border-radius: 0.25em;
            background: rgba(245, 158, 11, 0.35);
            color: inherit;
        }
        @media (prefers-color-scheme: dark) {
          mark{
            background: rgba(245, 158, 11, 0.28);
          }
        }

        .nk-link{
            opacity: 0.92;
            font-weight: 650;
        }

        /* í¬ë¡¤ë§ ìƒíƒœ ë°•ìŠ¤ */
        .nk-stat{
            display:flex;
            gap:10px;
            flex-wrap:wrap;
            margin-top: 6px;
        }
        .nk-pill{
            padding:6px 10px;
            border-radius:999px;
            border:1px solid rgba(128,128,128,0.25);
            background: rgba(34,197,94,0.10);
            font-weight:750;
            font-size:0.9rem;
        }
        .nk-pill-warn{
            background: rgba(245,158,11,0.12);
        }
        .nk-pill-bad{
            background: rgba(239,68,68,0.12);
        }
        @media (prefers-color-scheme: dark) {
          .nk-pill{
            border:1px solid rgba(255,255,255,0.18);
          }
        }
        </style>
        """,
        unsafe_allow_html=True
    )


# ============================================================
# 2) matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
# ============================================================
def setup_matplotlib_korean_font() -> None:
    try:
        fm.fontManager.addfont(FONT_PATH)
        plt.rcParams["font.family"] = fm.FontProperties(fname=FONT_PATH).get_name()
    except Exception:
        plt.rcParams["font.family"] = "Malgun Gothic"
    plt.rcParams["axes.unicode_minus"] = False


# ============================================================
# 3) ë¦¬ì†ŒìŠ¤ ë¡œë”©(ìºì‹œ)
# ============================================================
def load_json(path: str) -> dict:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


@st.cache_data(show_spinner=False)
def load_stopwords_file(path: str) -> set[str]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return {w.strip() for w in f if w.strip()}
    except Exception:
        return set()


@st.cache_resource
def load_tokenizer():
    try:
        with open(TOKENIZER_PATH, "rb") as f:
            return pickle.load(f)
    except Exception:
        return None


# ============================================================
# 4) í…ìŠ¤íŠ¸ ìœ í‹¸
# ============================================================
def clean_title(raw_title: str) -> str:
    t = html.unescape(raw_title or "")
    t = re.sub(r"<.*?>", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def format_pubdate(pub_date: str) -> str:
    try:
        dt = parsedate_to_datetime(pub_date)
        return dt.strftime("%Y-%m-%d %H:%M")
    except Exception:
        return pub_date or ""


@st.cache_data(show_spinner=False)
def clean_text_keep_korean(text: str) -> str:
    text = re.sub(r"\d|[a-zA-Z]|\W", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def normalize_token(t: str) -> str:
    if t is None:
        return ""
    t = str(t).strip()
    t = re.sub(r"[\"'â€œâ€â€˜â€™\(\)\[\]\{\},\.\!\?\:\;]", "", t)
    t = re.sub(r"\s+", "", t)
    return t


def build_final_keyword(category: str, user_keyword: str) -> str:
    category = (category or "").strip()
    user_keyword = re.sub(r"\s+", " ", (user_keyword or "")).strip()
    return f"{category} {user_keyword}".strip()


def safe_filename(s: str) -> str:
    s = s.strip()
    s = re.sub(r"[^\w\-ê°€-í£]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "result"


def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


def highlight_keyword(text: str, keyword: str) -> str:
    if not keyword:
        return text
    try:
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        return pattern.sub(lambda m: f"<mark>{m.group(0)}</mark>", text)
    except Exception:
        return text


# ============================================================
# 5) ë„¤ì´ë²„ API(ë°©ì–´ ì½”ë“œ)
# ============================================================
def naver_news_api_request(keyword: str, display: int, start: int, client_id: str, client_secret: str) -> list[dict]:
    if not client_id.strip() or not client_secret.strip():
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return []

    url = f"https://openapi.naver.com/v1/search/news.json?query={quote(keyword)}&display={display}&start={start}"
    headers = {
        "X-Naver-Client-Id": client_id.strip(),
        "X-Naver-Client-Secret": client_secret.strip(),
    }

    try:
        res = rq.get(url, headers=headers, timeout=10)
    except rq.exceptions.Timeout:
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤(timeout).")
        return []
    except rq.exceptions.ConnectionError:
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ConnectionError).")
        return []
    except rq.exceptions.RequestException as e:
        st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return []

    if res.status_code != 200:
        st.error("API ìš”ì²­ ì‹¤íŒ¨")
        if res.status_code in (401, 403):
            st.warning("API ì¸ì¦ ì‹¤íŒ¨(ê¶Œí•œ/í‚¤ ì˜¤ë¥˜). Client ID/Secretì„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            st.warning(f"HTTP ìƒíƒœì½”ë“œ: {res.status_code}")
        return []

    try:
        data = res.json()
        return data.get("items", []) or []
    except Exception:
        st.error("API ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨")
        return []


def fetch_news_items(final_keyword: str, total_display: int, client_id: str, client_secret: str) -> list[dict]:
    items: list[dict] = []
    page_count = max(1, total_display // 100)

    for i in range(page_count):
        start = 100 * i + 1
        page_items = naver_news_api_request(final_keyword, 100, start, client_id, client_secret)
        if page_items:
            items.extend(page_items)

    return items


def build_items_dataframe(items: list[dict]) -> pd.DataFrame:
    rows = []
    for it in items:
        rows.append({
            "title": clean_title(it.get("title", "")),
            "pubDate": format_pubdate(it.get("pubDate", "")),
            "link": it.get("link", ""),
        })
    return pd.DataFrame(rows)


# ============================================================
# 6) í¬ë¡¤ë§(ì‹¤íŒ¨ skip) + âœ… ì„±ê³µë¥ /ì›ì¸ ì§‘ê³„ìš© ìƒíƒœ ë°˜í™˜
# ============================================================
def crawl_naver_news_body_with_status(url: str) -> tuple[str, str, int | None]:
    """
    ë³¸ë¬¸ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ (body_text, status_label, http_code)ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    status_label ì˜ˆì‹œ:
    - ok
    - timeout
    - connection
    - request_exception
    - http_403
    - http_429
    - http_other
    - no_selector
    """
    try:
        res = rq.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=CRAWL_TIMEOUT)

        if res.status_code == 403:
            return "", "http_403", 403
        if res.status_code == 429:
            return "", "http_429", 429
        if res.status_code != 200:
            return "", "http_other", res.status_code

        soup = bs4.BeautifulSoup(res.text, "html.parser")
        tag = soup.select_one("#dic_area")
        if not tag:
            return "", "no_selector", res.status_code

        body = tag.get_text(separator=" ", strip=True)
        return body, "ok", res.status_code

    except rq.exceptions.Timeout:
        return "", "timeout", None
    except rq.exceptions.ConnectionError:
        return "", "connection", None
    except rq.exceptions.RequestException:
        return "", "request_exception", None
    except Exception:
        return "", "request_exception", None


def collect_corpus_from_items_with_stats(items: list[dict]) -> tuple[list[str], dict]:
    """
    âœ… (ê°œì„ ) ë³¸ë¬¸ ìˆ˜ì§‘ + ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ë¥¼ í•¨ê»˜ ë°˜í™˜

    ë°˜í™˜:
    - docs_clean: ë¶„ì„ ê°€ëŠ¥í•œ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸
    - stats: í¬ë¡¤ë§ ì„±ê³µë¥  íŒë‹¨ìš© dict
      {
        "total_items": int,
        "naver_links": int,
        "ok": int,
        "failed": int,
        "too_short": int,
        "no_selector": int,
        "timeout": int,
        "connection": int,
        "request_exception": int,
        "http_403": int,
        "http_429": int,
        "http_other": int,
        "by_reason": {reason: count, ...}
      }
    """
    docs_clean: list[str] = []
    reason_counter = Counter()

    total_items = len(items)
    naver_links = 0

    for it in items:
        link = it.get("link", "")
        if "n.news.naver" not in link:
            reason_counter["not_naver"] += 1
            continue

        naver_links += 1

        body, status, _http = crawl_naver_news_body_with_status(link)
        if status != "ok":
            reason_counter[status] += 1
            continue

        cleaned = clean_text_keep_korean(body)
        if len(cleaned) < MIN_BODY_LEN:
            reason_counter["too_short"] += 1
            continue

        reason_counter["ok"] += 1
        docs_clean.append(cleaned)

    ok = int(reason_counter.get("ok", 0))
    failed = naver_links - ok  # ë„¤ì´ë²„ ë§í¬ ì¤‘ ì„±ê³µ(ok) ì•„ë‹Œ ê²ƒ

    stats = {
        "total_items": total_items,
        "naver_links": naver_links,
        "ok": ok,
        "failed": max(0, failed),
        "too_short": int(reason_counter.get("too_short", 0)),
        "no_selector": int(reason_counter.get("no_selector", 0)),
        "timeout": int(reason_counter.get("timeout", 0)),
        "connection": int(reason_counter.get("connection", 0)),
        "request_exception": int(reason_counter.get("request_exception", 0)),
        "http_403": int(reason_counter.get("http_403", 0)),
        "http_429": int(reason_counter.get("http_429", 0)),
        "http_other": int(reason_counter.get("http_other", 0)),
        "not_naver": int(reason_counter.get("not_naver", 0)),
        "by_reason": dict(reason_counter),
    }

    return docs_clean, stats


def infer_crawl_root_cause(stats: dict) -> str:
    """
    í†µê³„ë¡œë¶€í„° "ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ì›ì¸"ì„ ê°„ë‹¨íˆ ì¶”ë¡ í•©ë‹ˆë‹¤.
    (ì •ë‹µ íŒì •ì€ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ, ë°°í¬ í™˜ê²½ì—ì„œ ë””ë²„ê¹… ë°©í–¥ì„ ì¡ëŠ” ìš©ë„)
    """
    naver_links = stats.get("naver_links", 0)
    if naver_links <= 0:
        return "ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤(ê²€ìƒ‰ ê²°ê³¼ê°€ ë‹¤ë¥¸ ì–¸ë¡ ì‚¬ ë§í¬ ìœ„ì£¼)."

    ok = stats.get("ok", 0)
    if ok == 0:
        # ì „ë¶€ ì‹¤íŒ¨ì¼ ë•Œ: ì–´ë–¤ ì‹¤íŒ¨ê°€ ì§€ë°°ì ì¸ì§€
        if stats.get("http_403", 0) + stats.get("http_429", 0) >= max(1, naver_links // 2):
            return "ì°¨ë‹¨/ë ˆì´íŠ¸ë¦¬ë°‹(403/429) ì˜ì‹¬: ë´‡ ì°¨ë‹¨ ë˜ëŠ” ìš”ì²­ ê³¼ë‹¤ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        if stats.get("timeout", 0) >= max(1, naver_links // 2):
            return "ë„¤íŠ¸ì›Œí¬ ì§€ì—°/íƒ€ì„ì•„ì›ƒ ìš°ì„¸: ë°°í¬ í™˜ê²½ ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” ëŒ€ìƒ ì„œë²„ ì‘ë‹µ ì§€ì—° ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        if stats.get("no_selector", 0) >= max(1, naver_links // 2):
            return "ë³¸ë¬¸ ì…€ë ‰í„°(#dic_area) ë¯¸ê²€ì¶œ ìš°ì„¸: ê¸°ì‚¬ DOM êµ¬ì¡° ë³€ê²½/ë‹¤ë¥¸ í˜ì´ì§€ ìœ í˜• ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        return "ë³µí•© ì›ì¸(ë„¤íŠ¸ì›Œí¬/ì°¨ë‹¨/íŒŒì‹±) ê°€ëŠ¥ì„±: ìƒì„¸ ì‹¤íŒ¨ ìœ í˜•ì„ í™•ì¸í•˜ì„¸ìš”."

    # ë¶€ë¶„ ì„±ê³µì¼ ë•Œ
    success_rate = ok / max(1, naver_links)
    if success_rate < 0.3:
        if stats.get("http_403", 0) + stats.get("http_429", 0) > stats.get("timeout", 0):
            return "ì„±ê³µë¥ ì´ ë‚®ê³  403/429ê°€ ë§ìŠµë‹ˆë‹¤: ì°¨ë‹¨/ìš”ì²­ ê³¼ë‹¤ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        if stats.get("timeout", 0) > stats.get("http_403", 0) + stats.get("http_429", 0):
            return "ì„±ê³µë¥ ì´ ë‚®ê³  timeoutì´ ë§ìŠµë‹ˆë‹¤: ë„¤íŠ¸ì›Œí¬ ì§€ì—°/ì‘ë‹µ ì§€ì—° ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        return "ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤: ì‹¤íŒ¨ ìœ í˜• ë¹„ìœ¨ì„ ë³´ê³  ì›ì¸ì„ ì¢í˜€ë³´ì„¸ìš”."

    return "í¬ë¡¤ë§ì€ ëŒ€ì²´ë¡œ ì •ìƒì…ë‹ˆë‹¤(í•„í„°/ë¶ˆìš©ì–´/ë¶„ëŸ‰ ì„¤ì •ì´ ê²°ê³¼ì— ë” í° ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)."


# ============================================================
# 7) ë¶„ì„(soynlp ëª…ì‚¬ set + TF-IDF)
# ============================================================
@st.cache_data(show_spinner=False)
def build_noun_set(docs_clean: list[str]) -> set[str]:
    sents = []
    for d in docs_clean:
        sents.extend([s.strip() for s in re.split(r"[\.!?]\s*|\n", d) if len(s.strip()) >= 10])

    if len(sents) < 5:
        return set()

    extractor = LRNounExtractor_v2(verbose=False)
    extractor.train(sents)
    nouns = extractor.extract()

    MIN_FREQ = 2
    MIN_SCORE = 0.4

    noun_set = set()
    for w, score in nouns.items():
        freq = getattr(score, "frequency", None)
        sc = getattr(score, "score", None)

        if freq is None and isinstance(score, dict):
            freq = score.get("frequency")
        if sc is None and isinstance(score, dict):
            sc = score.get("score")

        if freq is not None and freq < MIN_FREQ:
            continue
        if sc is not None and sc < MIN_SCORE:
            continue
        noun_set.add(w)

    return noun_set


def tokenize_and_filter_docs(docs_clean: list[str], stopwords: set[str]) -> list[list[str]]:
    tokenizer = load_tokenizer()
    noun_set = build_noun_set(docs_clean)

    docs_tokens: list[list[str]] = []

    for d in docs_clean:
        if tokenizer is None:
            raw_tokens = d.split()
        else:
            try:
                raw_tokens = [t1 for t1, _ in tokenizer.tokenize(d, flatten=False)]
            except Exception:
                raw_tokens = d.split()

        filtered = []
        for t in raw_tokens:
            nt = normalize_token(t)
            if not nt:
                continue
            if not (2 <= len(nt) <= 8):
                continue
            if nt in stopwords:
                continue
            if noun_set and (nt not in noun_set):
                continue
            filtered.append(nt)

        docs_tokens.append(filtered)

    return docs_tokens


def compute_tfidf_scores(docs_tokens: list[list[str]], top_k: int = 80) -> dict[str, float]:
    docs_str = [" ".join(ts) for ts in docs_tokens if ts]
    if len(docs_str) < 2:
        return {}

    try:
        vec = TfidfVectorizer(
            tokenizer=str.split,
            token_pattern=None,
            lowercase=False,
            min_df=2,
        )
        X = vec.fit_transform(docs_str)
        terms = np.array(vec.get_feature_names_out())
        scores = np.asarray(X.sum(axis=0)).ravel()

        if scores.size == 0:
            return {}

        idx = np.argsort(scores)[::-1][:top_k]
        return {terms[i]: float(scores[i]) for i in idx}
    except Exception:
        return {}


def build_keyword_tables(score_dict: dict[str, float]) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    df_kw = (
        pd.DataFrame(list(score_dict.items()), columns=["keyword", "score"])
        .sort_values("score", ascending=False)
    )
    return df_kw, df_kw.head(50).copy(), df_kw.head(20).copy()


# ============================================================
# 8) ì‹œê°í™”(ì´ë¯¸ì§€ bytes)
# ============================================================
def fig_to_png_bytes(fig) -> bytes:
    buf = BytesIO()
    fig.savefig(buf, format="png", dpi=160, bbox_inches="tight")
    buf.seek(0)
    return buf.getvalue()


def make_wordcloud_png(freq: dict[str, float], mask_name: str) -> bytes | None:
    if not freq:
        return None

    bg_path = MASK_BG.get(mask_name, MASK_BG["ì—†ìŒ"])
    mask = None
    try:
        img = Image.open(bg_path)
        mask = np.array(img)
    except Exception:
        mask = None

    wc = WordCloud(
        font_path=FONT_PATH,
        background_color="white",
        max_words=80,
        mask=mask,
    ).generate_from_frequencies(freq)

    fig = plt.figure(figsize=(10, 10))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")

    png = fig_to_png_bytes(fig)
    plt.close(fig)
    return png


def make_top20_bar_png(df_top20: pd.DataFrame) -> bytes | None:
    if df_top20.empty:
        return None

    fig = plt.figure(figsize=(10, 5))
    plt.bar(df_top20["keyword"], df_top20["score"])
    plt.xticks(rotation=45, ha="right")
    plt.title("TF-IDF ìƒìœ„ í‚¤ì›Œë“œ (Top 20)")
    plt.tight_layout()

    png = fig_to_png_bytes(fig)
    plt.close(fig)
    return png


def make_images_zip_bytes(wordcloud_png: bytes, top20_png: bytes, base_name: str) -> bytes:
    zip_buf = BytesIO()
    with zipfile.ZipFile(zip_buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr(f"{base_name}_wordcloud.png", wordcloud_png)
        zf.writestr(f"{base_name}_top20.png", top20_png)
    zip_buf.seek(0)
    return zip_buf.getvalue()


# ============================================================
# 9) UI ë Œë”ë§
# ============================================================
def render_header_with_lottie_and_center_title():
    col1, col2 = st.columns([1, 2.2])

    with col1:
        lottie = load_json(LOTTIE_PATH)
        if lottie:
            st_lottie(lottie, speed=1, loop=True, width=200, height=200)

    with col2:
        st.markdown(
            """
            <div class="nk-title-wrap">
                <div class="nk-title-main">ë‰´ìŠ¤ í‚¤ì›Œë“œ ì–´í”Œë¦¬ì¼€ì´ì…˜</div>
                <div class="nk-title-sub">(ë¶„ì„ &amp; ì‹œê°í™”)</div>
            </div>
            """,
            unsafe_allow_html=True
        )


def render_sidebar_api_settings():
    st.sidebar.header("API Keys :")
    st.session_state.setdefault("client_id", "")
    st.session_state.setdefault("client_secret", "")

    with st.sidebar.form("client_settings", clear_on_submit=False):
        cid = st.text_input("Client ID:", value=st.session_state["client_id"])
        secret = st.text_input("Client Secret:", type="password", value=st.session_state["client_secret"])
        if st.form_submit_button("OK"):
            st.session_state["client_id"] = (cid or "").strip()
            st.session_state["client_secret"] = (secret or "").strip()
            st.rerun()


def render_sidebar_options():
    st.sidebar.header("í‘œì‹œ/ë‹¤ìš´ë¡œë“œ ì˜µì…˜ :")
    r1c1, r1c2, r1c3 = st.sidebar.columns(3)
    with r1c1:
        show_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°", value=True, key="opt_show_articles")
    with r1c2:
        show_links = st.checkbox("ë§í¬ ì œê³µ", value=False, key="opt_show_links")
    with r1c3:
        dl_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)", value=False, key="opt_dl_articles")

    r2c1, r2c2, r2c3 = st.sidebar.columns(3)
    with r2c1:
        show_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë³´ê¸°", value=True, key="opt_show_keywords")
    with r2c2:
        dl_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)", value=False, key="opt_dl_keywords")
    with r2c3:
        dl_images = st.checkbox("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)", value=False, key="opt_dl_images")

    return {
        "show_articles": show_articles,
        "show_links": show_links,
        "dl_articles": dl_articles,
        "show_keywords": show_keywords,
        "dl_keywords": dl_keywords,
        "dl_images": dl_images,
    }


def render_sidebar_stopwords() -> set[str]:
    st.sidebar.header("ë¶ˆìš©ì–´(Stopwords) :")
    base_stop = load_stopwords_file(STOPWORDS_PATH)
    extra_stop = st.sidebar.text_area("ì¶”ê°€ ë¶ˆìš©ì–´(ì¤„ë°”ê¿ˆìœ¼ë¡œ ì…ë ¥)", value="", height=120)
    stopwords = base_stop | {w.strip() for w in extra_stop.splitlines() if w.strip()}
    st.sidebar.caption(f"í˜„ì¬ ë¶ˆìš©ì–´ ìˆ˜: {len(stopwords)} (íŒŒì¼ + ì¶”ê°€ ì…ë ¥)")
    return stopwords


def render_search_form():
    with st.container(border=True):
        st.subheader("ê²€ìƒ‰ ì¡°ê±´")

        with st.form("search", clear_on_submit=False):
            c1, c2, c3 = st.columns([1, 2, 1])

            with c1:
                category = st.selectbox("ë¶„ì•¼", ["ê²½ì œ", "ì •ì¹˜", "ì‚¬íšŒ", "êµ­ì œ", "ì—°ì˜ˆ", "IT", "ë¬¸í™”"])
            with c2:
                user_keyword = st.text_input(
                    "ê²€ìƒ‰ í‚¤ì›Œë“œ(í•„ìˆ˜)",
                    value="",
                    placeholder="ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI, ë©”íƒ€ë²„ìŠ¤ ..."
                )
            with c3:
                display = st.select_slider("ë¶„ëŸ‰", options=[100, 200, 300, 400, 500], value=100)

            mask = st.radio("ë°±ë§ˆìŠ¤í¬", ["ì—†ìŒ", "íƒ€ì›", "ë§í’ì„ ", "í•˜íŠ¸"], horizontal=True)
            submit = st.form_submit_button("ê²€ìƒ‰ ì‹¤í–‰")

    return {
        "category": category,
        "user_keyword": user_keyword,
        "display": display,
        "mask": mask,
        "submitted": submit,
    }


# ============================================================
# 10) ê²°ê³¼ ì„¸ì…˜ ì €ì¥/ì´ˆê¸°í™”
# ============================================================
def save_results_to_session(
    final_keyword: str,
    df_items: pd.DataFrame,
    df_kw_top50: pd.DataFrame,
    df_kw_top20: pd.DataFrame,
    wc_png: bytes,
    top20_png: bytes,
    zip_bytes: bytes,
    crawl_stats: dict,
):
    st.session_state["result_ready"] = True
    st.session_state["final_keyword"] = final_keyword
    st.session_state["df_items"] = df_items
    st.session_state["df_kw_top50"] = df_kw_top50
    st.session_state["df_kw_top20"] = df_kw_top20
    st.session_state["wc_png"] = wc_png
    st.session_state["top20_png"] = top20_png
    st.session_state["images_zip"] = zip_bytes
    st.session_state["crawl_stats"] = crawl_stats  # âœ… ì¶”ê°€


def clear_results_session():
    st.session_state["result_ready"] = False
    for k in ["final_keyword", "df_items", "df_kw_top50", "df_kw_top20", "wc_png", "top20_png", "images_zip", "crawl_stats"]:
        if k in st.session_state:
            del st.session_state[k]


# ============================================================
# 11) íŒŒì´í”„ë¼ì¸ ì‹¤í–‰(ìƒíƒœë°•ìŠ¤+ì§„í–‰ë°”)
# ============================================================
def run_pipeline(form: dict, stopwords: set[str], status_box, progress_bar):
    if not form["user_keyword"].strip():
        st.warning("ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI)")
        return

    client_id = st.session_state.get("client_id", "").strip()
    client_secret = st.session_state.get("client_secret", "").strip()
    if not client_id or not client_secret:
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì…ë ¥ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return

    final_keyword = build_final_keyword(form["category"], form["user_keyword"])

    # 1) API ìˆ˜ì§‘
    status_box.info(f"1/4 ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ ì¤‘... (ê²€ìƒ‰ì–´: {final_keyword})")
    progress_bar.progress(0.2)
    items = fetch_news_items(final_keyword, form["display"], client_id, client_secret)

    if not items:
        status_box.error("ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.info("ê°€ëŠ¥í•œ ì›ì¸: (1) ì¸ì¦ ì‹¤íŒ¨ (2) ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (3) ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return

    df_items = build_items_dataframe(items)

    # 2) í¬ë¡¤ë§ + âœ… í†µê³„ ìˆ˜ì§‘
    status_box.info("2/4 ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
    progress_bar.progress(0.45)
    docs_clean, crawl_stats = collect_corpus_from_items_with_stats(items)

    # âœ… í†µê³„/ì¶”ë¡ ì„ ìƒíƒœë°•ìŠ¤ ì•„ë˜ ì¦‰ì‹œ ë³´ì—¬ì£¼ë©´ ë°°í¬ ë””ë²„ê¹…ì´ ì‰¬ì›€
    ok = crawl_stats.get("ok", 0)
    naver_links = max(1, crawl_stats.get("naver_links", 0))
    success_rate = ok / naver_links * 100

    status_box.info(f"í¬ë¡¤ë§ ì„±ê³µë¥ : {success_rate:.1f}% (ì„±ê³µ {ok} / ë„¤ì´ë²„ë§í¬ {crawl_stats.get('naver_links', 0)})")

    # 2-1) ë°ì´í„° ë¶€ì¡±
    if len(docs_clean) < 5:
        status_box.warning("ë³¸ë¬¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.")
        st.info(
            "ê°œì„  íŒ:\n"
            "- ë¶„ëŸ‰ì„ 300~500ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”.\n"
            "- í‚¤ì›Œë“œë¥¼ ë” ì¼ë°˜ì ìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”.\n"
            "- ê¸°ì‚¬ ëª©ë¡ì—ì„œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•´ë³´ì„¸ìš”."
        )
        # âœ… ë¶€ì¡±í•œ ì´ìœ ë„ ê°™ì´ í‘œì‹œ
        st.warning(f"ì›ì¸ ì¶”ì •: {infer_crawl_root_cause(crawl_stats)}")
        st.session_state["crawl_stats_preview"] = crawl_stats
        return

    # 3) ë¶„ì„
    status_box.info("3/4 í‚¤ì›Œë“œ ë¶„ì„ ì¤‘(ëª…ì‚¬ í•„í„° + TF-IDF)...")
    progress_bar.progress(0.7)
    docs_tokens = tokenize_and_filter_docs(docs_clean, stopwords)

    score_dict = compute_tfidf_scores(docs_tokens, top_k=80)
    if not score_dict:
        status_box.warning("í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ë°ì´í„°/í•„í„° ì¡°ê±´ ë¶€ì¡±).")
        st.info("ê°œì„  íŒ: ë¶„ëŸ‰ì„ ëŠ˜ë¦¬ê±°ë‚˜ ë¶ˆìš©ì–´ë¥¼ ê³¼ë„í•˜ê²Œ ì¶”ê°€í•˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.warning(f"ì›ì¸ ì¶”ì •(í¬ë¡¤ë§ ê´€ì ): {infer_crawl_root_cause(crawl_stats)}")
        st.session_state["crawl_stats_preview"] = crawl_stats
        return

    _, df_kw_top50, df_kw_top20 = build_keyword_tables(score_dict)

    # 4) ì‹œê°í™” ìƒì„±
    status_box.info("4/4 ì‹œê°í™” ìƒì„± ì¤‘...")
    progress_bar.progress(0.9)

    wc_png = make_wordcloud_png(score_dict, form["mask"])
    top20_png = make_top20_bar_png(df_kw_top20)

    if not wc_png or not top20_png:
        status_box.error("ì‹œê°í™” ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤(ë°ì´í„° ë¶€ì¡±/ë Œë”ë§ ì˜¤ë¥˜).")
        st.warning(f"ì›ì¸ ì¶”ì •(í¬ë¡¤ë§ ê´€ì ): {infer_crawl_root_cause(crawl_stats)}")
        st.session_state["crawl_stats_preview"] = crawl_stats
        return

    base = safe_filename(final_keyword)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_bytes = make_images_zip_bytes(wc_png, top20_png, f"{base}_{ts}")

    progress_bar.progress(1.0)
    status_box.success("ì™„ë£Œ! ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    save_results_to_session(
        final_keyword=final_keyword,
        df_items=df_items,
        df_kw_top50=df_kw_top50,
        df_kw_top20=df_kw_top20,
        wc_png=wc_png,
        top20_png=top20_png,
        zip_bytes=zip_bytes,
        crawl_stats=crawl_stats,
    )


# ============================================================
# 12) ê²°ê³¼ íƒ­ UI
# ============================================================
def render_top5_badges(df_kw_top50: pd.DataFrame) -> None:
    top5 = df_kw_top50.head(5)["keyword"].tolist()
    badges = "".join([f'<span class="nk-badge">#{kw}</span>' for kw in top5])

    st.markdown(
        f"""
        <div style="margin:6px 0 14px 0;">
            <div style="font-weight:800; margin-bottom:6px;">í•µì‹¬ í‚¤ì›Œë“œ ìš”ì•½</div>
            <div>{badges}</div>
        </div>
        """,
        unsafe_allow_html=True
    )


def render_crawl_stats_panel(crawl_stats: dict) -> None:
    """
    âœ… í¬ë¡¤ë§ ì„±ê³µë¥ /ì‹¤íŒ¨ ì›ì¸ì„ UIë¡œ ë³´ì—¬ì£¼ëŠ” íŒ¨ë„
    - í•œ ì¤„ ìš”ì•½ + ì‹¤íŒ¨ìœ í˜•ë³„ ì¹´ìš´íŠ¸ + ì›ì¸ ì¶”ì •
    """
    if not crawl_stats:
        return

    total_items = crawl_stats.get("total_items", 0)
    naver_links = crawl_stats.get("naver_links", 0)
    ok = crawl_stats.get("ok", 0)
    failed = crawl_stats.get("failed", 0)

    success_rate = (ok / max(1, naver_links)) * 100

    # ë¹ ë¥¸ ì§„ë‹¨ìš© í•µì‹¬ ì¹´ìš´íŠ¸
    http_block = crawl_stats.get("http_403", 0) + crawl_stats.get("http_429", 0)
    timeout_cnt = crawl_stats.get("timeout", 0)
    selector_cnt = crawl_stats.get("no_selector", 0)
    too_short = crawl_stats.get("too_short", 0)

    st.markdown(
        f"""
        <div class="nk-card">
          <div style="font-weight:850; font-size:1.05rem;">í¬ë¡¤ë§ ì„±ê³µë¥ </div>
          <div class="nk-stat">
            <span class="nk-pill">ì„±ê³µë¥  {success_rate:.1f}%</span>
            <span class="nk-pill">ì„±ê³µ {ok}</span>
            <span class="nk-pill nk-pill-bad">ì‹¤íŒ¨ {failed}</span>
            <span class="nk-pill nk-pill-warn">ë„¤ì´ë²„ë§í¬ {naver_links}</span>
            <span class="nk-pill">ì „ì²´ items {total_items}</span>
          </div>
          <div style="margin-top:10px; opacity:0.85; font-weight:750;">
            ì›ì¸ ì¶”ì •: {infer_crawl_root_cause(crawl_stats)}
          </div>
        </div>
        """,
        unsafe_allow_html=True
    )

    with st.expander("ìƒì„¸ ì‹¤íŒ¨ ìœ í˜• ë³´ê¸°(ë””ë²„ê¹…ìš©)"):
        c1, c2, c3, c4 = st.columns(4)
        with c1:
            st.metric("403/429(ì°¨ë‹¨ ì˜ì‹¬)", http_block)
        with c2:
            st.metric("timeout", timeout_cnt)
        with c3:
            st.metric("#dic_area ì—†ìŒ", selector_cnt)
        with c4:
            st.metric("ë„ˆë¬´ ì§§ì€ ë³¸ë¬¸", too_short)

        detail_rows = []
        by_reason = crawl_stats.get("by_reason", {})
        for k, v in sorted(by_reason.items(), key=lambda x: x[1], reverse=True):
            detail_rows.append({"reason": k, "count": v})

        if detail_rows:
            st.dataframe(pd.DataFrame(detail_rows), use_container_width=True)


def render_results_tabs(options: dict, user_keyword: str) -> None:
    if not st.session_state.get("result_ready", False):
        # íŒŒì´í”„ë¼ì¸ ì¤‘ê°„ ì‹¤íŒ¨ ì‹œì—ë„ preview í†µê³„ë§Œ ë³´ì—¬ì£¼ê³  ì‹¶ë‹¤ë©´
        preview = st.session_state.get("crawl_stats_preview")
        if preview:
            st.info("ì´ì „ ì‹¤í–‰ì—ì„œ í¬ë¡¤ë§ í†µê³„ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            render_crawl_stats_panel(preview)
        else:
            st.info("ê²€ìƒ‰ ì‹¤í–‰ í›„ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")
        return

    final_keyword = st.session_state["final_keyword"]
    df_items: pd.DataFrame = st.session_state["df_items"]
    df_kw_top50: pd.DataFrame = st.session_state["df_kw_top50"]
    df_kw_top20: pd.DataFrame = st.session_state["df_kw_top20"]

    wc_png: bytes = st.session_state["wc_png"]
    top20_png: bytes = st.session_state["top20_png"]
    images_zip: bytes = st.session_state["images_zip"]
    crawl_stats: dict = st.session_state.get("crawl_stats", {})

    tab_summary, tab_articles, tab_keywords = st.tabs(["ìš”ì•½", "ê¸°ì‚¬ ëª©ë¡", "í‚¤ì›Œë“œ í‘œ"])

    # ---------------------------
    # ìš”ì•½ íƒ­
    # ---------------------------
    with tab_summary:
        st.subheader(f"ë¶„ì„ ìš”ì•½: {final_keyword}")

        # âœ… í¬ë¡¤ë§ ì„±ê³µë¥  íŒ¨ë„
        render_crawl_stats_panel(crawl_stats)

        if not df_kw_top50.empty:
            render_top5_badges(df_kw_top50)

        left, right = st.columns(2)
        with left:
            st.caption("ì›Œë“œí´ë¼ìš°ë“œ")
            st.image(wc_png, use_container_width=True)
        with right:
            st.caption("Top20 ë§‰ëŒ€ì°¨íŠ¸")
            st.image(top20_png, use_container_width=True)

        with st.container(border=True):
            st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")

            can_articles = not df_items.empty
            can_keywords = not df_kw_top50.empty
            can_images = bool(images_zip)

            base = safe_filename(final_keyword)
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")

            b1, b2, b3 = st.columns(3)
            with b1:
                st.download_button(
                    label="ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)",
                    data=df_to_csv_bytes(df_items) if can_articles else b"",
                    file_name=f"articles_{base}_{ts}.csv",
                    mime="text/csv",
                    disabled=not (options["dl_articles"] and can_articles),
                )
            with b2:
                st.download_button(
                    label="í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)",
                    data=df_to_csv_bytes(df_kw_top50) if can_keywords else b"",
                    file_name=f"keywords_{base}_{ts}.csv",
                    mime="text/csv",
                    disabled=not (options["dl_keywords"] and can_keywords),
                )
            with b3:
                st.download_button(
                    label="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)",
                    data=images_zip if can_images else b"",
                    file_name=f"images_{base}_{ts}.zip",
                    mime="application/zip",
                    disabled=not (options["dl_images"] and can_images),
                )

            st.caption("â€» ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œëŠ” ì›Œë“œí´ë¼ìš°ë“œ+Top20 PNGë¥¼ ZIPìœ¼ë¡œ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.")

    # ---------------------------
    # ê¸°ì‚¬ ëª©ë¡ íƒ­
    # ---------------------------
    with tab_articles:
        st.subheader("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡")

        if df_items.empty:
            st.warning("ê¸°ì‚¬ ëª©ë¡ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        fcol1, fcol2 = st.columns([1, 2])
        with fcol1:
            sort_order = st.selectbox("ì •ë ¬", ["ìµœì‹ ìˆœ", "ì˜¤ë˜ëœìˆœ"], index=0)
        with fcol2:
            title_filter = st.text_input("ì œëª©ì— í¬í•¨ëœ ë‹¨ì–´ í•„í„°", value="")

        df_view = df_items.copy()
        df_view["__dt"] = pd.to_datetime(df_view["pubDate"], errors="coerce")
        df_view = df_view.sort_values("__dt", ascending=(sort_order == "ì˜¤ë˜ëœìˆœ"))
        if title_filter.strip():
            df_view = df_view[df_view["title"].str.contains(title_filter, case=False, na=False)]
        df_view = df_view.drop(columns="__dt")

        st.divider()

        if not options["show_articles"]:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ 'ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°'ë¥¼ ì¼œë©´ í‘œì‹œë©ë‹ˆë‹¤.")
            return

        highlight_key = user_keyword.strip()
        MAX_SHOW = 60
        df_show = df_view.head(MAX_SHOW)

        st.caption(f"í‘œì‹œ ê¸°ì‚¬ ìˆ˜: {len(df_show)} / í•„í„°ë§ í›„ ì „ì²´: {len(df_view)}")

        for _, row in df_show.iterrows():
            title = row.get("title", "")
            pub = row.get("pubDate", "")
            link = row.get("link", "")

            title_html = highlight_keyword(title, highlight_key)

            st.markdown(
                f"""
                <div class="nk-card">
                    <div style="font-weight:800; font-size:16px; line-height:1.35;">
                        {title_html}
                    </div>
                    <div style="opacity:0.75; font-size:13px; margin-top:4px;">
                        {pub}
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )

            if options["show_links"] and link:
                st.markdown(f'- <a class="nk-link" href="{link}" target="_blank">ğŸ”— ê¸°ì‚¬ ë°”ë¡œê°€ê¸°</a>', unsafe_allow_html=True)

        if len(df_view) > MAX_SHOW:
            st.info(f"ê¸°ì‚¬ ëª©ë¡ì´ ë§ì•„ ìƒìœ„ {MAX_SHOW}ê°œë§Œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤. (í•„í„°ë¥¼ ë” ê±¸ì–´ë³´ì„¸ìš”)")

    # ---------------------------
    # í‚¤ì›Œë“œ í‘œ íƒ­
    # ---------------------------
    with tab_keywords:
        st.subheader("í‚¤ì›Œë“œ(TF-IDF) ìƒìœ„ 50")

        if df_kw_top50.empty:
            st.warning("í‚¤ì›Œë“œ í‘œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        if options["show_keywords"]:
            st.dataframe(df_kw_top50, use_container_width=True)
        else:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ 'í‚¤ì›Œë“œ í‘œ ë³´ê¸°'ë¥¼ ì¼œë©´ í‘œì‹œë©ë‹ˆë‹¤.")


# ============================================================
# 13) ì•± ì‹¤í–‰(ë©”ì¸)
# ============================================================
def run_app():
    st.set_page_config(page_title="ë‰´ìŠ¤ í‚¤ì›Œë“œ ì–´í”Œë¦¬ì¼€ì´ì…˜", layout="wide")

    inject_theme_friendly_css()
    setup_matplotlib_korean_font()
    render_header_with_lottie_and_center_title()

    render_sidebar_api_settings()

    # âœ… ì‚¬ì´ë“œë°”: ì˜µì…˜ -> ë¶ˆìš©ì–´
    options = render_sidebar_options()
    stopwords = render_sidebar_stopwords()

    form = render_search_form()

    status_box = st.empty()
    progress_bar = st.progress(0)

    st.session_state.setdefault("result_ready", False)

    if form["submitted"]:
        clear_results_session()
        # ì´ì „ ì‹¤í–‰ì—ì„œ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” preview ì œê±°
        if "crawl_stats_preview" in st.session_state:
            del st.session_state["crawl_stats_preview"]

        progress_bar.progress(0)
        run_pipeline(form, stopwords, status_box, progress_bar)

    render_results_tabs(options, user_keyword=form["user_keyword"])


if __name__ == "__main__":
    run_app()
