#
# ============================================================
#  News Keyword Visualizer V4 (Diagnostics Enhanced)
# ------------------------------------------------------------
#  âœ… V4 ê¸°ëŠ¥ ìœ ì§€ + ë°°í¬ í™˜ê²½ ì§„ë‹¨ UI ì¶”ê°€
#
#  [ì¶”ê°€ëœ ì§„ë‹¨ ê¸°ëŠ¥]
#  - API ë‹¨ê³„(í˜ì´ì§€ ë‹¨ìœ„) ì„±ê³µ/ì‹¤íŒ¨, ìƒíƒœì½”ë“œ, timeout, í‰ê·  ì‘ë‹µì‹œê°„ í‘œì‹œ
#  - í¬ë¡¤ë§ ë‹¨ê³„ ì„±ê³µ/ì‹¤íŒ¨/ìŠ¤í‚µ(ì§§ìŒ/ë„¤ì´ë²„ ë§í¬ ì•„ë‹˜) ì¹´ìš´íŠ¸ í‘œì‹œ
#
#  ì‹¤í–‰:
#     streamlit run app_v4.py
# ============================================================
#

# ============================================================
# ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ
# ============================================================
import json
import re
import pickle
import html
import time
from datetime import datetime
from email.utils import parsedate_to_datetime
from io import BytesIO
from urllib.parse import quote
import zipfile

import requests as rq
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

import bs4
import pandas as pd
import numpy as np

import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from PIL import Image
from wordcloud import WordCloud
from streamlit_lottie import st_lottie

from sklearn.feature_extraction.text import TfidfVectorizer
from soynlp.noun import LRNounExtractor_v2


# ============================================================
# 0) ì „ì—­ ì„¤ì •(ê²½ë¡œ/íŒŒì¼)
# ============================================================
FONT_PATH = "./resources/NanumSquareR.ttf"
STOPWORDS_PATH = "./resources/stopwords_ko.txt"
TOKENIZER_PATH = "./resources/my_tokenizer1.model"
LOTTIE_PATH = "./resources/lottie-full-movie-experience-including-music-news-video-weather-and-lots-of-entertainment.json"

MASK_BG = {
    "ì—†ìŒ": "./resources/background_0.png",
    "íƒ€ì›": "./resources/background_1.png",
    "ë§í’ì„ ": "./resources/background_2.png",
    "í•˜íŠ¸": "./resources/background_3.png",
}

# API/í¬ë¡¤ë§ timeout ê¸°ë³¸ê°’(ë°°í¬ í™˜ê²½ì—ì„œ read_timeoutì´ ë” ì¤‘ìš”)
API_TIMEOUT = (5, 25)      # (connect_timeout, read_timeout)
CRAWL_TIMEOUT = (5, 15)    # í¬ë¡¤ë§ì€ ë„ˆë¬´ ê¸¸ê²Œ ëŒì§€ ì•Šê²Œ


# ============================================================
# 1) í…Œë§ˆ ì¹œí™” CSS (ë¼ì´íŠ¸/ë‹¤í¬ ê³µìš©)
# ============================================================
def inject_theme_friendly_css() -> None:
    """
    ë¼ì´íŠ¸/ë‹¤í¬ ëª¨ë“œ ëª¨ë‘ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì´ë„ë¡
    - ì¹´ë“œ/ë°°ì§€/í•˜ì´ë¼ì´íŠ¸ ìŠ¤íƒ€ì¼ì„ í…Œë§ˆ ì¹œí™”ì ìœ¼ë¡œ ì ìš©
    """
    st.markdown(
        """
        <style>
        .nk-title-wrap{
            text-align:center;
            margin: 0.25rem 0 1.0rem 0;
        }
        .nk-title-main{
            font-size: 2.0rem;
            font-weight: 850;
            line-height: 1.1;
            margin: 0;
        }
        .nk-title-sub{
            font-size: 1.1rem;
            font-weight: 700;
            opacity: 0.8;
            margin-top: 0.35rem;
        }

        .nk-card{
            border: 1px solid rgba(128,128,128,0.25);
            border-radius: 14px;
            padding: 12px 14px;
            margin: 10px 0;
            background: rgba(128,128,128,0.06);
        }
        @media (prefers-color-scheme: dark) {
          .nk-card{
            background: rgba(255,255,255,0.04);
            border: 1px solid rgba(255,255,255,0.18);
          }
        }

        .nk-badge{
            display:inline-block;
            padding:6px 12px;
            margin:4px 6px 4px 0;
            border-radius: 16px;
            font-size: 0.92rem;
            font-weight: 750;
            border: 1px solid rgba(128,128,128,0.25);
            background: rgba(99,102,241,0.10);
        }
        @media (prefers-color-scheme: dark) {
          .nk-badge{
            background: rgba(99,102,241,0.18);
            border: 1px solid rgba(255,255,255,0.18);
          }
        }

        mark{
            padding: 0.08em 0.18em;
            border-radius: 0.25em;
            background: rgba(245, 158, 11, 0.35);
            color: inherit;
        }
        @media (prefers-color-scheme: dark) {
          mark{ background: rgba(245, 158, 11, 0.28); }
        }

        .nk-link{
            opacity: 0.92;
            font-weight: 650;
        }

        /* ì§„ë‹¨ ë°°ì§€ */
        .nk-pill{
            display:inline-block;
            padding:6px 10px;
            margin:4px 6px 4px 0;
            border-radius: 999px;
            font-size: 0.86rem;
            font-weight: 750;
            border: 1px solid rgba(128,128,128,0.25);
            background: rgba(16,185,129,0.12); /* emerald */
        }
        @media (prefers-color-scheme: dark) {
          .nk-pill{
            background: rgba(16,185,129,0.18);
            border: 1px solid rgba(255,255,255,0.18);
          }
        }

        .nk-pill-warn{
            background: rgba(245,158,11,0.12);
        }
        @media (prefers-color-scheme: dark) {
          .nk-pill-warn{ background: rgba(245,158,11,0.18); }
        }

        .nk-pill-bad{
            background: rgba(239,68,68,0.12);
        }
        @media (prefers-color-scheme: dark) {
          .nk-pill-bad{ background: rgba(239,68,68,0.18); }
        }

        </style>
        """,
        unsafe_allow_html=True
    )


# ============================================================
# 2) matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
# ============================================================
def setup_matplotlib_korean_font() -> None:
    """matplotlib í•œê¸€ ê¹¨ì§ ë°©ì§€ ì„¤ì •."""
    try:
        fm.fontManager.addfont(FONT_PATH)
        plt.rcParams["font.family"] = fm.FontProperties(fname=FONT_PATH).get_name()
    except Exception:
        plt.rcParams["font.family"] = "Malgun Gothic"
    plt.rcParams["axes.unicode_minus"] = False


# ============================================================
# 3) requests ì„¸ì…˜ + ì¬ì‹œë„(ë°°í¬ í™˜ê²½ ì•ˆì •í™”)
# ============================================================
@st.cache_resource
def get_http_session() -> rq.Session:
    """
    ë°°í¬ í™˜ê²½ì—ì„œ timeout/ì¼ì‹œ ì¥ì• ê°€ ì¢…ì¢… ë°œìƒí•©ë‹ˆë‹¤.
    - Session ì¬ì‚¬ìš©(ì—°ê²°/TLS ì¬ì‚¬ìš©)
    - Retry + Backoff(429/5xx/timeout ê°€ì¹˜ ìˆëŠ” ì˜¤ë¥˜ ìë™ ë³µêµ¬)
    """
    session = rq.Session()

    retry = Retry(
        total=4,
        connect=4,
        read=4,
        backoff_factor=0.6,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"],
        raise_on_status=False,
        respect_retry_after_header=True,
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)
    session.mount("https://", adapter)
    session.mount("http://", adapter)

    session.headers.update({"User-Agent": "Mozilla/5.0 (compatible; NKVisualizer/1.0)"})
    return session


# ============================================================
# 4) ë¦¬ì†ŒìŠ¤ ë¡œë”©(ìºì‹œ)
# ============================================================
def load_json(path: str) -> dict:
    """JSON íŒŒì¼ ì•ˆì „ ë¡œë“œ(ì‹¤íŒ¨ ì‹œ ë¹ˆ dict)."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


@st.cache_data(show_spinner=False)
def load_stopwords_file(path: str) -> set[str]:
    """ë¶ˆìš©ì–´ íŒŒì¼ -> set (ì‹¤íŒ¨ ì‹œ ë¹ˆ set)."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return {w.strip() for w in f if w.strip()}
    except Exception:
        return set()


@st.cache_resource
def load_tokenizer():
    """í† í¬ë‚˜ì´ì € ë¡œë“œ(ì‹¤íŒ¨ ì‹œ None)."""
    try:
        with open(TOKENIZER_PATH, "rb") as f:
            return pickle.load(f)
    except Exception:
        return None


# ============================================================
# 5) í…ìŠ¤íŠ¸ ìœ í‹¸
# ============================================================
def clean_title(raw_title: str) -> str:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ titleì˜ HTML ì œê±° + ê³µë°± ì •ë¦¬."""
    t = html.unescape(raw_title or "")
    t = re.sub(r"<.*?>", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def format_pubdate(pub_date: str) -> str:
    """pubDateë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¬¸ìì—´ë¡œ ë³€í™˜."""
    try:
        dt = parsedate_to_datetime(pub_date)
        return dt.strftime("%Y-%m-%d %H:%M")
    except Exception:
        return pub_date or ""


@st.cache_data(show_spinner=False)
def clean_text_keep_korean(text: str) -> str:
    """ìˆ«ì/ì˜ë¬¸/íŠ¹ìˆ˜ë¬¸ì ì œê±° + ê³µë°± ì •ë¦¬(í•œê¸€ ì¤‘ì‹¬)."""
    text = re.sub(r"\d|[a-zA-Z]|\W", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def normalize_token(t: str) -> str:
    """í† í° ì •ê·œí™”(êµ¬ë‘ì /ê³µë°± ì œê±°)."""
    if t is None:
        return ""
    t = str(t).strip()
    t = re.sub(r"[\"'â€œâ€â€˜â€™\(\)\[\]\{\},\.\!\?\:\;]", "", t)
    t = re.sub(r"\s+", "", t)
    return t


def build_final_keyword(category: str, user_keyword: str) -> str:
    """ë¶„ì•¼ + ì‚¬ìš©ì í‚¤ì›Œë“œ ê²°í•©."""
    category = (category or "").strip()
    user_keyword = re.sub(r"\s+", " ", (user_keyword or "")).strip()
    return f"{category} {user_keyword}".strip()


def safe_filename(s: str) -> str:
    """íŒŒì¼ëª… ì•ˆì „í™”."""
    s = s.strip()
    s = re.sub(r"[^\w\-ê°€-í£]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "result"


def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    """CSV ë‹¤ìš´ë¡œë“œìš© bytes(utf-8-sig)."""
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


def highlight_keyword(text: str, keyword: str) -> str:
    """ì œëª© ë‚´ í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸(<mark>)."""
    if not keyword:
        return text
    try:
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        return pattern.sub(lambda m: f"<mark>{m.group(0)}</mark>", text)
    except Exception:
        return text


# ============================================================
# 6) ì§„ë‹¨ìš© ë©”íŠ¸ë¦­(ì´ˆê¸°í™”/ì €ì¥)
# ============================================================
def init_api_metrics(total_display: int) -> dict:
    """API ë‹¨ê³„ ì§„ë‹¨ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”."""
    page_cnt = max(1, total_display // 100)
    return {
        "pages_planned": page_cnt,
        "pages_attempted": 0,
        "pages_ok": 0,
        "pages_fail": 0,
        "items_total": 0,
        "http_status_counts": {},     # ì˜ˆ: {200: 3, 429: 1}
        "error_counts": {},           # ì˜ˆ: {"ReadTimeout":2, "AuthFail":1}
        "latencies": [],              # ì‘ë‹µì‹œê°„(ì´ˆ) ë¦¬ìŠ¤íŠ¸
        "last_error": "",
    }


def init_crawl_metrics() -> dict:
    """í¬ë¡¤ë§ ë‹¨ê³„ ì§„ë‹¨ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”."""
    return {
        "candidate_links": 0,         # n.news.naver ë§í¬ ìˆ˜
        "attempted": 0,
        "success": 0,
        "fail": 0,
        "skip_short": 0,              # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ skip
        "skip_non_naver": 0,          # ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ê°€ ì•„ë‹ˆë¼ skip
        "last_error": "",
    }


def inc_dict(d: dict, k, inc: int = 1):
    d[k] = d.get(k, 0) + inc


def save_results_to_session(
    final_keyword: str,
    df_items: pd.DataFrame,
    df_kw_top50: pd.DataFrame,
    df_kw_top20: pd.DataFrame,
    wc_png: bytes,
    top20_png: bytes,
    zip_bytes: bytes,
    api_metrics: dict,
    crawl_metrics: dict,
):
    st.session_state["result_ready"] = True
    st.session_state["final_keyword"] = final_keyword
    st.session_state["df_items"] = df_items
    st.session_state["df_kw_top50"] = df_kw_top50
    st.session_state["df_kw_top20"] = df_kw_top20
    st.session_state["wc_png"] = wc_png
    st.session_state["top20_png"] = top20_png
    st.session_state["images_zip"] = zip_bytes
    st.session_state["api_metrics"] = api_metrics
    st.session_state["crawl_metrics"] = crawl_metrics


def clear_results_session():
    st.session_state["result_ready"] = False
    for k in [
        "final_keyword", "df_items", "df_kw_top50", "df_kw_top20",
        "wc_png", "top20_png", "images_zip", "api_metrics", "crawl_metrics"
    ]:
        if k in st.session_state:
            del st.session_state[k]


# ============================================================
# 7) ë„¤ì´ë²„ API(ë°©ì–´ + ì§„ë‹¨)
# ============================================================
def naver_news_api_request(
    keyword: str,
    display: int,
    start: int,
    client_id: str,
    client_secret: str,
    api_metrics: dict,
) -> list[dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API(1í˜ì´ì§€).
    âœ… ì§„ë‹¨ í¬í•¨:
    - í˜ì´ì§€ ì‹œë„/ì„±ê³µ/ì‹¤íŒ¨
    - ìƒíƒœì½”ë“œ ì¹´ìš´íŠ¸
    - timeout/exception ì¹´ìš´íŠ¸
    - ì‘ë‹µì‹œê°„ ê¸°ë¡
    """
    api_metrics["pages_attempted"] += 1

    if not client_id.strip() or not client_secret.strip():
        inc_dict(api_metrics["error_counts"], "MissingKey")
        api_metrics["last_error"] = "MissingKey"
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return []

    url = (
        "https://openapi.naver.com/v1/search/news.json"
        f"?query={quote(keyword)}&display={display}&start={start}"
    )
    headers = {
        "X-Naver-Client-Id": client_id.strip(),
        "X-Naver-Client-Secret": client_secret.strip(),
    }

    session = get_http_session()

    try:
        t0 = time.perf_counter()
        res = session.get(url, headers=headers, timeout=API_TIMEOUT)
        elapsed = time.perf_counter() - t0
        api_metrics["latencies"].append(elapsed)

    except rq.exceptions.ConnectTimeout:
        api_metrics["pages_fail"] += 1
        inc_dict(api_metrics["error_counts"], "ConnectTimeout")
        api_metrics["last_error"] = "ConnectTimeout"
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ì„œë²„ ì—°ê²° ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤(ConnectTimeout).")
        return []
    except rq.exceptions.ReadTimeout:
        api_metrics["pages_fail"] += 1
        inc_dict(api_metrics["error_counts"], "ReadTimeout")
        api_metrics["last_error"] = "ReadTimeout"
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ì‘ë‹µ ëŒ€ê¸° ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤(ReadTimeout).")
        return []
    except rq.exceptions.Timeout:
        api_metrics["pages_fail"] += 1
        inc_dict(api_metrics["error_counts"], "Timeout")
        api_metrics["last_error"] = "Timeout"
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤(timeout).")
        return []
    except rq.exceptions.ConnectionError:
        api_metrics["pages_fail"] += 1
        inc_dict(api_metrics["error_counts"], "ConnectionError")
        api_metrics["last_error"] = "ConnectionError"
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ConnectionError).")
        return []
    except rq.exceptions.RequestException as e:
        api_metrics["pages_fail"] += 1
        inc_dict(api_metrics["error_counts"], "RequestException")
        api_metrics["last_error"] = f"RequestException: {e}"
        st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return []

    inc_dict(api_metrics["http_status_counts"], res.status_code)

    # ìš”êµ¬ì‚¬í•­: 200 ì•„ë‹ˆë©´ "API ìš”ì²­ ì‹¤íŒ¨"
    if res.status_code != 200:
        api_metrics["pages_fail"] += 1
        st.error("API ìš”ì²­ ì‹¤íŒ¨")

        if res.status_code in (401, 403):
            inc_dict(api_metrics["error_counts"], "AuthFail")
            api_metrics["last_error"] = f"AuthFail({res.status_code})"
            st.warning("API ì¸ì¦ ì‹¤íŒ¨(ê¶Œí•œ/í‚¤ ì˜¤ë¥˜). Client ID/Secretì„ í™•ì¸í•˜ì„¸ìš”.")
        elif res.status_code == 429:
            inc_dict(api_metrics["error_counts"], "RateLimit(429)")
            api_metrics["last_error"] = "RateLimit(429)"
            st.warning("ìš”ì²­ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤(429). ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        else:
            inc_dict(api_metrics["error_counts"], f"HTTP({res.status_code})")
            api_metrics["last_error"] = f"HTTP({res.status_code})"
            hint = (res.text or "")[:200].strip()
            if hint:
                st.caption(f"ì‘ë‹µ ì¼ë¶€: {hint}")
            st.warning(f"HTTP ìƒíƒœì½”ë“œ: {res.status_code}")

        return []

    try:
        data = res.json()
        items = data.get("items", []) or []
        api_metrics["pages_ok"] += 1
        api_metrics["items_total"] += len(items)
        return items
    except Exception:
        api_metrics["pages_fail"] += 1
        inc_dict(api_metrics["error_counts"], "JSONParseFail")
        api_metrics["last_error"] = "JSONParseFail"
        st.error("API ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨")
        return []


def fetch_news_items(final_keyword: str, total_display: int, client_id: str, client_secret: str, api_metrics: dict) -> list[dict]:
    """
    100ë‹¨ìœ„ë¡œ í˜ì´ì§€ ìš”ì²­ í›„ items í•©ì¹˜ê¸°.
    âœ… ë°°í¬ ì•ˆì •í™”:
    - ì¼ë¶€ í˜ì´ì§€ ì‹¤íŒ¨í•´ë„ ê³„ì†
    - ì—°ì† ì‹¤íŒ¨ ëˆ„ì ë˜ë©´ ì¡°ê¸° ì¤‘ë‹¨(ì•±ì´ ì˜¤ë˜ ë©ˆì¶˜ ë“¯ ë³´ì´ëŠ” ë¬¸ì œ ë°©ì§€)
    """
    items: list[dict] = []
    page_count = api_metrics["pages_planned"]

    consecutive_fail = 0
    MAX_CONSEC_FAIL = 2

    for i in range(page_count):
        start = 100 * i + 1
        page_items = naver_news_api_request(
            final_keyword, 100, start,
            client_id, client_secret,
            api_metrics
        )

        if page_items:
            items.extend(page_items)
            consecutive_fail = 0
        else:
            consecutive_fail += 1
            if consecutive_fail >= MAX_CONSEC_FAIL:
                st.warning("API ìš”ì²­ì´ ì—°ì†ìœ¼ë¡œ ì‹¤íŒ¨í•˜ì—¬ ì¶”ê°€ í˜ì´ì§€ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break

    return items


def build_items_dataframe(items: list[dict]) -> pd.DataFrame:
    """itemsì—ì„œ title/pubDate/linkë§Œ ì¶”ì¶œ."""
    rows = []
    for it in items:
        rows.append({
            "title": clean_title(it.get("title", "")),
            "pubDate": format_pubdate(it.get("pubDate", "")),
            "link": it.get("link", ""),
        })
    return pd.DataFrame(rows)


# ============================================================
# 8) í¬ë¡¤ë§(ì‹¤íŒ¨ skip + ì§„ë‹¨)
# ============================================================
def crawl_naver_news_body(url: str, crawl_metrics: dict) -> str:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§(#dic_area).
    âœ… ì§„ë‹¨ í¬í•¨:
    - attempted/success/fail ì¹´ìš´íŠ¸
    - ì˜ˆì™¸ ë°œìƒí•´ë„ ì•±ì´ ì£½ì§€ ì•Šê²Œ ì²˜ë¦¬
    """
    crawl_metrics["attempted"] += 1
    session = get_http_session()

    try:
        res = session.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=CRAWL_TIMEOUT)
        if res.status_code != 200:
            crawl_metrics["fail"] += 1
            crawl_metrics["last_error"] = f"HTTP({res.status_code})"
            return ""

        soup = bs4.BeautifulSoup(res.text, "html.parser")
        tag = soup.select_one("#dic_area")
        if not tag:
            crawl_metrics["fail"] += 1
            crawl_metrics["last_error"] = "NoSelector(#dic_area)"
            return ""

        crawl_metrics["success"] += 1
        return tag.get_text(separator=" ", strip=True)

    except rq.exceptions.Timeout:
        crawl_metrics["fail"] += 1
        crawl_metrics["last_error"] = "Timeout"
        return ""
    except rq.exceptions.RequestException as e:
        crawl_metrics["fail"] += 1
        crawl_metrics["last_error"] = f"RequestException: {e}"
        return ""
    except Exception as e:
        crawl_metrics["fail"] += 1
        crawl_metrics["last_error"] = f"Exception: {e}"
        return ""


def collect_corpus_from_items(items: list[dict], crawl_metrics: dict) -> list[str]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ë§Œ ë³¸ë¬¸ ìˆ˜ì§‘.
    - ì‹¤íŒ¨/ì§§ì€ ë³¸ë¬¸ skip
    - ì§„ë‹¨ìš© ì¹´ìš´íŠ¸ ì§‘ê³„
    """
    docs = []
    for it in items:
        link = it.get("link", "")

        if "n.news.naver" not in link:
            crawl_metrics["skip_non_naver"] += 1
            continue

        crawl_metrics["candidate_links"] += 1

        body = crawl_naver_news_body(link, crawl_metrics)
        if not body:
            continue

        cleaned = clean_text_keep_korean(body)
        if len(cleaned) < 100:
            crawl_metrics["skip_short"] += 1
            continue

        docs.append(cleaned)

    return docs


# ============================================================
# 9) ë¶„ì„(soynlp ëª…ì‚¬ set + TF-IDF)
# ============================================================
@st.cache_data(show_spinner=False)
def build_noun_set(docs_clean: list[str]) -> set[str]:
    """soynlpë¡œ ëª…ì‚¬ í›„ë³´ set ìƒì„±(ë°ì´í„° ì ìœ¼ë©´ ë¹ˆ set)."""
    sents = []
    for d in docs_clean:
        sents.extend([s.strip() for s in re.split(r"[\.!?]\s*|\n", d) if len(s.strip()) >= 10])

    if len(sents) < 5:
        return set()

    extractor = LRNounExtractor_v2(verbose=False)
    extractor.train(sents)
    nouns = extractor.extract()

    MIN_FREQ = 2
    MIN_SCORE = 0.4

    noun_set = set()
    for w, score in nouns.items():
        freq = getattr(score, "frequency", None)
        sc = getattr(score, "score", None)

        if freq is None and isinstance(score, dict):
            freq = score.get("frequency")
        if sc is None and isinstance(score, dict):
            sc = score.get("score")

        if freq is not None and freq < MIN_FREQ:
            continue
        if sc is not None and sc < MIN_SCORE:
            continue
        noun_set.add(w)

    return noun_set


def tokenize_and_filter_docs(docs_clean: list[str], stopwords: set[str]) -> list[list[str]]:
    """
    í† í°í™” -> ëª…ì‚¬ í•„í„°(ê°€ëŠ¥í•˜ë©´) -> ë¶ˆìš©ì–´ ì œê±°
    - í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ ì‹œ split fallback
    - ëª…ì‚¬ setì´ ë¹„ë©´(ë°ì´í„° ë¶€ì¡±) ëª…ì‚¬ í•„í„° ì™„í™”
    """
    tokenizer = load_tokenizer()
    noun_set = build_noun_set(docs_clean)

    docs_tokens: list[list[str]] = []

    for d in docs_clean:
        if tokenizer is None:
            raw_tokens = d.split()
        else:
            try:
                raw_tokens = [t1 for t1, _ in tokenizer.tokenize(d, flatten=False)]
            except Exception:
                raw_tokens = d.split()

        filtered = []
        for t in raw_tokens:
            nt = normalize_token(t)
            if not nt:
                continue
            if not (2 <= len(nt) <= 8):
                continue
            if nt in stopwords:
                continue
            if noun_set and (nt not in noun_set):
                continue
            filtered.append(nt)

        docs_tokens.append(filtered)

    return docs_tokens


def compute_tfidf_scores(docs_tokens: list[list[str]], top_k: int = 80) -> dict[str, float]:
    """TF-IDF ì ìˆ˜ ê³„ì‚°(ì˜¤ë¥˜/ë¶€ì¡± ì‹œ ë¹ˆ dict)."""
    docs_str = [" ".join(ts) for ts in docs_tokens if ts]
    if len(docs_str) < 2:
        return {}

    try:
        vec = TfidfVectorizer(
            tokenizer=str.split,
            token_pattern=None,
            lowercase=False,
            min_df=2,
        )
        X = vec.fit_transform(docs_str)
        terms = np.array(vec.get_feature_names_out())
        scores = np.asarray(X.sum(axis=0)).ravel()

        if scores.size == 0:
            return {}

        idx = np.argsort(scores)[::-1][:top_k]
        return {terms[i]: float(scores[i]) for i in idx}
    except Exception:
        return {}


def build_keyword_tables(score_dict: dict[str, float]) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """ì ìˆ˜ dict -> DataFrame + Top50/Top20."""
    df_kw = (
        pd.DataFrame(list(score_dict.items()), columns=["keyword", "score"])
        .sort_values("score", ascending=False)
    )
    return df_kw, df_kw.head(50).copy(), df_kw.head(20).copy()


# ============================================================
# 10) ì‹œê°í™”(PNG bytes)
# ============================================================
def fig_to_png_bytes(fig) -> bytes:
    """matplotlib figure -> PNG bytes."""
    buf = BytesIO()
    fig.savefig(buf, format="png", dpi=160, bbox_inches="tight")
    buf.seek(0)
    return buf.getvalue()


def make_wordcloud_png(freq: dict[str, float], mask_name: str) -> bytes | None:
    """ì›Œë“œí´ë¼ìš°ë“œ PNG bytes ìƒì„±."""
    if not freq:
        return None

    bg_path = MASK_BG.get(mask_name, MASK_BG["ì—†ìŒ"])
    mask = None
    try:
        img = Image.open(bg_path)
        mask = np.array(img)
    except Exception:
        mask = None

    wc = WordCloud(
        font_path=FONT_PATH,
        background_color="white",
        max_words=80,
        mask=mask,
    ).generate_from_frequencies(freq)

    fig = plt.figure(figsize=(10, 10))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")

    png = fig_to_png_bytes(fig)
    plt.close(fig)
    return png


def make_top20_bar_png(df_top20: pd.DataFrame) -> bytes | None:
    """Top20 ë§‰ëŒ€ì°¨íŠ¸ PNG bytes ìƒì„±."""
    if df_top20.empty:
        return None

    fig = plt.figure(figsize=(10, 5))
    plt.bar(df_top20["keyword"], df_top20["score"])
    plt.xticks(rotation=45, ha="right")
    plt.title("TF-IDF ìƒìœ„ í‚¤ì›Œë“œ (Top 20)")
    plt.tight_layout()

    png = fig_to_png_bytes(fig)
    plt.close(fig)
    return png


def make_images_zip_bytes(wordcloud_png: bytes, top20_png: bytes, base_name: str) -> bytes:
    """ì›Œë“œí´ë¼ìš°ë“œ + Top20 ì´ë¯¸ì§€ë¥¼ ZIPìœ¼ë¡œ ë¬¶ì–´ì„œ ë°˜í™˜."""
    zip_buf = BytesIO()
    with zipfile.ZipFile(zip_buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr(f"{base_name}_wordcloud.png", wordcloud_png)
        zf.writestr(f"{base_name}_top20.png", top20_png)
    zip_buf.seek(0)
    return zip_buf.getvalue()


# ============================================================
# 11) UI ë Œë”ë§
# ============================================================
def render_header_with_lottie_and_center_title():
    """Lottie ì¢Œì¸¡ + 2ì¤„ ê°€ìš´ë° íƒ€ì´í‹€"""
    col1, col2 = st.columns([1, 2.2])

    with col1:
        lottie = load_json(LOTTIE_PATH)
        if lottie:
            st_lottie(lottie, speed=1, loop=True, width=200, height=200)

    with col2:
        st.markdown(
            """
            <div class="nk-title-wrap">
                <div class="nk-title-main">ë‰´ìŠ¤ í‚¤ì›Œë“œ ì–´í”Œë¦¬ì¼€ì´ì…˜</div>
                <div class="nk-title-sub">(ë¶„ì„ &amp; ì‹œê°í™”)</div>
            </div>
            """,
            unsafe_allow_html=True
        )


def render_sidebar_api_settings():
    """ì‚¬ì´ë“œë°” API ì„¤ì • í¼."""
    st.sidebar.header("API Keys :")
    st.session_state.setdefault("client_id", "")
    st.session_state.setdefault("client_secret", "")

    with st.sidebar.form("client_settings", clear_on_submit=False):
        cid = st.text_input("Client ID:", value=st.session_state["client_id"])
        secret = st.text_input("Client Secret:", type="password", value=st.session_state["client_secret"])
        if st.form_submit_button("OK"):
            st.session_state["client_id"] = (cid or "").strip()
            st.session_state["client_secret"] = (secret or "").strip()
            st.rerun()


def render_sidebar_options():
    """
    ì˜µì…˜ ì²´í¬ë°•ìŠ¤(ìš”êµ¬ì‚¬í•­):
    - 1ì¤„: ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°, ë§í¬ ì œê³µ, ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)
    - 2ì¤„: í‚¤ì›Œë“œ í‘œ ë³´ê¸°, í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv), ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)
    """
    st.sidebar.header("í‘œì‹œ/ë‹¤ìš´ë¡œë“œ ì˜µì…˜ :")
    r1c1, r1c2, r1c3 = st.sidebar.columns(3)
    with r1c1:
        show_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°", value=True, key="opt_show_articles")
    with r1c2:
        show_links = st.checkbox("ë§í¬ ì œê³µ", value=False, key="opt_show_links")
    with r1c3:
        dl_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)", value=False, key="opt_dl_articles")

    r2c1, r2c2, r2c3 = st.sidebar.columns(3)
    with r2c1:
        show_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë³´ê¸°", value=True, key="opt_show_keywords")
    with r2c2:
        dl_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)", value=False, key="opt_dl_keywords")
    with r2c3:
        dl_images = st.checkbox("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)", value=False, key="opt_dl_images")

    return {
        "show_articles": show_articles,
        "show_links": show_links,
        "dl_articles": dl_articles,
        "show_keywords": show_keywords,
        "dl_keywords": dl_keywords,
        "dl_images": dl_images,
    }


def render_sidebar_stopwords() -> set[str]:
    """ë¶ˆìš©ì–´ ì˜ì—­(íŒŒì¼ + ì¶”ê°€ ì…ë ¥)."""
    st.sidebar.header("ë¶ˆìš©ì–´(Stopwords) :")
    base_stop = load_stopwords_file(STOPWORDS_PATH)
    extra_stop = st.sidebar.text_area("ì¶”ê°€ ë¶ˆìš©ì–´(ì¤„ë°”ê¿ˆìœ¼ë¡œ ì…ë ¥)", value="", height=120)
    stopwords = base_stop | {w.strip() for w in extra_stop.splitlines() if w.strip()}
    st.sidebar.caption(f"í˜„ì¬ ë¶ˆìš©ì–´ ìˆ˜: {len(stopwords)} (íŒŒì¼ + ì¶”ê°€ ì…ë ¥)")
    return stopwords


def render_search_form():
    """ê²€ìƒ‰ í¼(UI ì¹´ë“œ)."""
    with st.container(border=True):
        st.subheader("ê²€ìƒ‰ ì¡°ê±´")

        with st.form("search", clear_on_submit=False):
            c1, c2, c3 = st.columns([1, 2, 1])

            with c1:
                category = st.selectbox("ë¶„ì•¼", ["ê²½ì œ", "ì •ì¹˜", "ì‚¬íšŒ", "êµ­ì œ", "ì—°ì˜ˆ", "IT", "ë¬¸í™”"])
            with c2:
                user_keyword = st.text_input(
                    "ê²€ìƒ‰ í‚¤ì›Œë“œ(í•„ìˆ˜)",
                    value="",
                    placeholder="ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI, ë©”íƒ€ë²„ìŠ¤ ..."
                )
            with c3:
                display = st.select_slider("ë¶„ëŸ‰", options=[100, 200, 300, 400, 500], value=100)

            mask = st.radio("ë°±ë§ˆìŠ¤í¬", ["ì—†ìŒ", "íƒ€ì›", "ë§í’ì„ ", "í•˜íŠ¸"], horizontal=True)
            submit = st.form_submit_button("ê²€ìƒ‰ ì‹¤í–‰")

    return {
        "category": category,
        "user_keyword": user_keyword,
        "display": display,
        "mask": mask,
        "submitted": submit,
    }


# ============================================================
# 12) íŒŒì´í”„ë¼ì¸ ì‹¤í–‰(ìƒíƒœë°•ìŠ¤+ì§„í–‰ë°”)
# ============================================================
def run_pipeline(form: dict, stopwords: set[str], status_box, progress_bar):
    """
    íŒŒì´í”„ë¼ì¸:
    1) API ìˆ˜ì§‘(+ì§„ë‹¨)
    2) í¬ë¡¤ë§(+ì§„ë‹¨)
    3) ë¶„ì„
    4) ì‹œê°í™”
    """
    if not form["user_keyword"].strip():
        st.warning("ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI)")
        return

    client_id = st.session_state.get("client_id", "").strip()
    client_secret = st.session_state.get("client_secret", "").strip()
    if not client_id or not client_secret:
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì…ë ¥ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return

    final_keyword = build_final_keyword(form["category"], form["user_keyword"])

    # âœ… ì§„ë‹¨ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
    api_metrics = init_api_metrics(form["display"])
    crawl_metrics = init_crawl_metrics()

    # 1) API ìˆ˜ì§‘
    status_box.info(f"1/4 ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ ì¤‘... (ê²€ìƒ‰ì–´: {final_keyword})")
    progress_bar.progress(0.2)

    items = fetch_news_items(final_keyword, form["display"], client_id, client_secret, api_metrics)

    # API ë‹¨ê³„ê°€ ì‹¤íŒ¨í•˜ë©´(=itemsê°€ ì—†ë‹¤ë©´) ì§„ë‹¨ ì •ë³´ë¥¼ ì €ì¥í•´ë‘ê³  ì¢…ë£Œ
    if not items:
        status_box.error("ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.info("ê°€ëŠ¥í•œ ì›ì¸: (1) ì¸ì¦ ì‹¤íŒ¨ (2) ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (3) ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

        # âœ… ê²°ê³¼ ì„¸ì…˜ì—ë„ ì§„ë‹¨ ì •ë³´ë§Œ ì €ì¥(ìš”ì•½ íƒ­ì—ì„œ í™•ì¸ ê°€ëŠ¥í•˜ê²Œ)
        df_items = pd.DataFrame(columns=["title", "pubDate", "link"])
        save_results_to_session(
            final_keyword=final_keyword,
            df_items=df_items,
            df_kw_top50=pd.DataFrame(columns=["keyword", "score"]),
            df_kw_top20=pd.DataFrame(columns=["keyword", "score"]),
            wc_png=b"",
            top20_png=b"",
            zip_bytes=b"",
            api_metrics=api_metrics,
            crawl_metrics=crawl_metrics,
        )
        st.session_state["result_ready"] = True
        return

    df_items = build_items_dataframe(items)

    # 2) í¬ë¡¤ë§
    status_box.info("2/4 ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
    progress_bar.progress(0.45)

    docs_clean = collect_corpus_from_items(items, crawl_metrics)

    if len(docs_clean) < 5:
        status_box.warning("ë³¸ë¬¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.")
        st.info(
            "ê°œì„  íŒ:\n"
            "- ë¶„ëŸ‰ì„ 300~500ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”.\n"
            "- í‚¤ì›Œë“œë¥¼ ë” ì¼ë°˜ì ìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”.\n"
            "- ê¸°ì‚¬ ëª©ë¡ì—ì„œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•´ë³´ì„¸ìš”.\n"
            "- (ë°°í¬ í™˜ê²½) í¬ë¡¤ë§ ì„±ê³µë¥ ì´ ë‚®ë‹¤ë©´ ì°¨ë‹¨ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤."
        )

        save_results_to_session(
            final_keyword=final_keyword,
            df_items=df_items,
            df_kw_top50=pd.DataFrame(columns=["keyword", "score"]),
            df_kw_top20=pd.DataFrame(columns=["keyword", "score"]),
            wc_png=b"",
            top20_png=b"",
            zip_bytes=b"",
            api_metrics=api_metrics,
            crawl_metrics=crawl_metrics,
        )
        st.session_state["result_ready"] = True
        return

    # 3) ë¶„ì„
    status_box.info("3/4 í‚¤ì›Œë“œ ë¶„ì„ ì¤‘(ëª…ì‚¬ í•„í„° + TF-IDF)...")
    progress_bar.progress(0.7)

    docs_tokens = tokenize_and_filter_docs(docs_clean, stopwords)

    score_dict = compute_tfidf_scores(docs_tokens, top_k=80)
    if not score_dict:
        status_box.warning("í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ë°ì´í„°/í•„í„° ì¡°ê±´ ë¶€ì¡±).")
        st.info("ê°œì„  íŒ: ë¶„ëŸ‰ì„ ëŠ˜ë¦¬ê±°ë‚˜ ë¶ˆìš©ì–´ë¥¼ ê³¼ë„í•˜ê²Œ ì¶”ê°€í•˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

        save_results_to_session(
            final_keyword=final_keyword,
            df_items=df_items,
            df_kw_top50=pd.DataFrame(columns=["keyword", "score"]),
            df_kw_top20=pd.DataFrame(columns=["keyword", "score"]),
            wc_png=b"",
            top20_png=b"",
            zip_bytes=b"",
            api_metrics=api_metrics,
            crawl_metrics=crawl_metrics,
        )
        st.session_state["result_ready"] = True
        return

    _, df_kw_top50, df_kw_top20 = build_keyword_tables(score_dict)

    # 4) ì‹œê°í™” ìƒì„±
    status_box.info("4/4 ì‹œê°í™” ìƒì„± ì¤‘...")
    progress_bar.progress(0.9)

    wc_png = make_wordcloud_png(score_dict, form["mask"])
    top20_png = make_top20_bar_png(df_kw_top20)

    if not wc_png or not top20_png:
        status_box.error("ì‹œê°í™” ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤(ë°ì´í„° ë¶€ì¡±/ë Œë”ë§ ì˜¤ë¥˜).")

        save_results_to_session(
            final_keyword=final_keyword,
            df_items=df_items,
            df_kw_top50=df_kw_top50,
            df_kw_top20=df_kw_top20,
            wc_png=b"",
            top20_png=b"",
            zip_bytes=b"",
            api_metrics=api_metrics,
            crawl_metrics=crawl_metrics,
        )
        st.session_state["result_ready"] = True
        return

    base = safe_filename(final_keyword)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_bytes = make_images_zip_bytes(wc_png, top20_png, f"{base}_{ts}")

    progress_bar.progress(1.0)
    status_box.success("ì™„ë£Œ! ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    save_results_to_session(
        final_keyword=final_keyword,
        df_items=df_items,
        df_kw_top50=df_kw_top50,
        df_kw_top20=df_kw_top20,
        wc_png=wc_png,
        top20_png=top20_png,
        zip_bytes=zip_bytes,
        api_metrics=api_metrics,
        crawl_metrics=crawl_metrics,
    )


# ============================================================
# 13) ê²°ê³¼ íƒ­ UI
# ============================================================
def render_top5_badges(df_kw_top50: pd.DataFrame) -> None:
    """ìš”ì•½ íƒ­ì— Top5 í‚¤ì›Œë“œë¥¼ ë°°ì§€ë¡œ ë Œë”ë§."""
    top5 = df_kw_top50.head(5)["keyword"].tolist()
    badges = "".join([f'<span class="nk-badge">#{kw}</span>' for kw in top5])

    st.markdown(
        f"""
        <div style="margin:6px 0 14px 0;">
            <div style="font-weight:800; margin-bottom:6px;">í•µì‹¬ í‚¤ì›Œë“œ ìš”ì•½</div>
            <div>{badges}</div>
        </div>
        """,
        unsafe_allow_html=True
    )


def render_diagnostics_panel(api_metrics: dict, crawl_metrics: dict):
    """
    âœ… ë°°í¬ ì§„ë‹¨ìš© íŒ¨ë„
    - API: í˜ì´ì§€ ì„±ê³µë¥ , ìƒíƒœì½”ë“œ ë¶„í¬, timeout/ì—ëŸ¬, í‰ê·  ì‘ë‹µì‹œê°„
    - Crawl: í›„ë³´ ë§í¬/ì‹œë„/ì„±ê³µ/ì‹¤íŒ¨/ìŠ¤í‚µ ì¹´ìš´íŠ¸
    """
    st.markdown('<div class="nk-card">', unsafe_allow_html=True)
    st.markdown("### ë°°í¬ í™˜ê²½ ì§„ë‹¨(ì›ì¸ ë¹ ë¥¸ íŒë³„)")

    # ---------- API ìš”ì•½ ----------
    pages_planned = api_metrics.get("pages_planned", 0)
    pages_attempted = api_metrics.get("pages_attempted", 0)
    pages_ok = api_metrics.get("pages_ok", 0)
    pages_fail = api_metrics.get("pages_fail", 0)
    items_total = api_metrics.get("items_total", 0)

    lat = api_metrics.get("latencies", [])
    avg_latency = float(np.mean(lat)) if lat else 0.0
    p95_latency = float(np.percentile(lat, 95)) if len(lat) >= 3 else (max(lat) if lat else 0.0)

    http_counts = api_metrics.get("http_status_counts", {})
    err_counts = api_metrics.get("error_counts", {})
    last_err = api_metrics.get("last_error", "")

    # ì„±ê³µë¥ 
    api_success_rate = (pages_ok / pages_attempted * 100) if pages_attempted else 0.0

    # ìƒíƒœ íŒë‹¨(ëŒ€ì¶© 3ë‹¨ê³„)
    # - ok: ì„±ê³µë¥  70% ì´ìƒ
    # - warn: 30~70%
    # - bad: 30% ë¯¸ë§Œ
    if api_success_rate >= 70:
        api_pill_class = "nk-pill"
        api_level = "ì–‘í˜¸"
    elif api_success_rate >= 30:
        api_pill_class = "nk-pill nk-pill-warn"
        api_level = "ì£¼ì˜"
    else:
        api_pill_class = "nk-pill nk-pill-bad"
        api_level = "ìœ„í—˜"

    st.markdown("**1) API ë‹¨ê³„(ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API)**")
    st.markdown(
        f"""
        <span class="{api_pill_class}">ì„±ê³µë¥  {api_success_rate:.0f}% ({api_level})</span>
        <span class="nk-pill">í˜ì´ì§€ OK {pages_ok}</span>
        <span class="nk-pill nk-pill-warn">í˜ì´ì§€ FAIL {pages_fail}</span>
        <span class="nk-pill">items {items_total}</span>
        <span class="nk-pill">í‰ê· ì‘ë‹µ {avg_latency:.2f}s</span>
        <span class="nk-pill">p95 {p95_latency:.2f}s</span>
        """,
        unsafe_allow_html=True
    )
    st.caption(f"ê³„íš í˜ì´ì§€: {pages_planned} / ì‹œë„ í˜ì´ì§€: {pages_attempted} / ë§ˆì§€ë§‰ ì—ëŸ¬: {last_err or '-'}")

    # ìƒíƒœì½”ë“œ/ì—ëŸ¬ í…Œì´ë¸”(ê°„ë‹¨)
    colA, colB = st.columns(2)
    with colA:
        if http_counts:
            df_http = pd.DataFrame(sorted(http_counts.items()), columns=["status_code", "count"])
            st.dataframe(df_http, use_container_width=True, height=150)
        else:
            st.info("ìƒíƒœì½”ë“œ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    with colB:
        if err_counts:
            df_err = pd.DataFrame(sorted(err_counts.items(), key=lambda x: x[1], reverse=True), columns=["error", "count"])
            st.dataframe(df_err, use_container_width=True, height=150)
        else:
            st.info("ì—ëŸ¬ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

    st.divider()

    # ---------- Crawl ìš”ì•½ ----------
    st.markdown("**2) í¬ë¡¤ë§ ë‹¨ê³„(ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘)**")
    candidate = crawl_metrics.get("candidate_links", 0)
    attempted = crawl_metrics.get("attempted", 0)
    success = crawl_metrics.get("success", 0)
    fail = crawl_metrics.get("fail", 0)
    skip_short = crawl_metrics.get("skip_short", 0)
    skip_non = crawl_metrics.get("skip_non_naver", 0)
    last_crawl_err = crawl_metrics.get("last_error", "")

    crawl_success_rate = (success / attempted * 100) if attempted else 0.0

    if crawl_success_rate >= 70:
        crawl_pill_class = "nk-pill"
        crawl_level = "ì–‘í˜¸"
    elif crawl_success_rate >= 30:
        crawl_pill_class = "nk-pill nk-pill-warn"
        crawl_level = "ì£¼ì˜"
    else:
        crawl_pill_class = "nk-pill nk-pill-bad"
        crawl_level = "ìœ„í—˜"

    st.markdown(
        f"""
        <span class="{crawl_pill_class}">ì„±ê³µë¥  {crawl_success_rate:.0f}% ({crawl_level})</span>
        <span class="nk-pill">í›„ë³´ë§í¬ {candidate}</span>
        <span class="nk-pill">ì‹œë„ {attempted}</span>
        <span class="nk-pill">ì„±ê³µ {success}</span>
        <span class="nk-pill nk-pill-warn">ì‹¤íŒ¨ {fail}</span>
        <span class="nk-pill nk-pill-warn">ì§§ì•„ì„œ ìŠ¤í‚µ {skip_short}</span>
        <span class="nk-pill">ë¹„ë„¤ì´ë²„ ìŠ¤í‚µ {skip_non}</span>
        """,
        unsafe_allow_html=True
    )
    st.caption(f"ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì—ëŸ¬: {last_crawl_err or '-'}")

    # ì›ì¸ íŒíŠ¸
    st.markdown("**ì›ì¸ íŒíŠ¸(ë¹ ë¥¸ íŒë‹¨)**")
    hints = []
    if api_success_rate < 30:
        hints.append("- API ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ â†’ ë°°í¬ ë„¤íŠ¸ì›Œí¬/ë ˆì´íŠ¸ë¦¬ë°‹/ì¸ì¦ ë¬¸ì œ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.")
    if api_success_rate >= 70 and crawl_success_rate < 30:
        hints.append("- APIëŠ” ì •ìƒì¸ë° í¬ë¡¤ë§ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ â†’ í¬ë¡¤ë§ ì°¨ë‹¨(ë´‡ ì°¨ë‹¨) ë˜ëŠ” ë³¸ë¬¸ ì„ íƒì ë³€í™” ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.")
    if candidate == 0:
        hints.append("- ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬(n.news.naver)ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤ â†’ ê²€ìƒ‰ ê²°ê³¼ ë§í¬ê°€ ë‹¤ë¥¸ ë„ë©”ì¸ ìœ„ì£¼ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if not hints:
        hints.append("- í° ì´ìƒ ì§•í›„ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ëŸ‰ì„ ëŠ˜ë¦¬ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”.")
    st.markdown("\n".join(hints))

    st.markdown("</div>", unsafe_allow_html=True)


def render_results_tabs(options: dict, user_keyword: str) -> None:
    """íƒ­ ê¸°ë°˜ ê²°ê³¼ UI: ìš”ì•½ / ê¸°ì‚¬ ëª©ë¡ / í‚¤ì›Œë“œ í‘œ"""
    if not st.session_state.get("result_ready", False):
        st.info("ê²€ìƒ‰ ì‹¤í–‰ í›„ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")
        return

    final_keyword = st.session_state.get("final_keyword", "")
    df_items: pd.DataFrame = st.session_state.get("df_items", pd.DataFrame())
    df_kw_top50: pd.DataFrame = st.session_state.get("df_kw_top50", pd.DataFrame())
    df_kw_top20: pd.DataFrame = st.session_state.get("df_kw_top20", pd.DataFrame())

    wc_png: bytes = st.session_state.get("wc_png", b"")
    top20_png: bytes = st.session_state.get("top20_png", b"")
    images_zip: bytes = st.session_state.get("images_zip", b"")

    api_metrics: dict = st.session_state.get("api_metrics", {})
    crawl_metrics: dict = st.session_state.get("crawl_metrics", {})

    tab_summary, tab_articles, tab_keywords = st.tabs(["ìš”ì•½", "ê¸°ì‚¬ ëª©ë¡", "í‚¤ì›Œë“œ í‘œ"])

    # ---------------------------
    # ìš”ì•½ íƒ­
    # ---------------------------
    with tab_summary:
        st.subheader(f"ë¶„ì„ ìš”ì•½: {final_keyword}")

        # âœ… ì§„ë‹¨ íŒ¨ë„(í•­ìƒ í‘œì‹œ)
        if api_metrics or crawl_metrics:
            render_diagnostics_panel(api_metrics, crawl_metrics)

        # í‚¤ì›Œë“œê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ë°©ì–´
        if not df_kw_top50.empty:
            render_top5_badges(df_kw_top50)

        # ì‹œê°í™”ëŠ” ìƒì„±ëœ ê²½ìš°ë§Œ
        if wc_png and top20_png:
            left, right = st.columns(2)
            with left:
                st.caption("ì›Œë“œí´ë¼ìš°ë“œ")
                st.image(wc_png, use_container_width=True)
            with right:
                st.caption("Top20 ë§‰ëŒ€ì°¨íŠ¸")
                st.image(top20_png, use_container_width=True)
        else:
            st.info("ì‹œê°í™” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (API/í¬ë¡¤ë§/ë¶„ì„ ë‹¨ê³„ì—ì„œ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

        # ë‹¤ìš´ë¡œë“œ ì•¡ì…˜ë°”(1ì¤„ 3ë²„íŠ¼)
        with st.container(border=True):
            st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")

            can_articles = not df_items.empty
            can_keywords = not df_kw_top50.empty
            can_images = bool(images_zip)

            base = safe_filename(final_keyword or "result")
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")

            b1, b2, b3 = st.columns(3)
            with b1:
                st.download_button(
                    label="ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)",
                    data=df_to_csv_bytes(df_items) if can_articles else b"",
                    file_name=f"articles_{base}_{ts}.csv",
                    mime="text/csv",
                    disabled=not (options["dl_articles"] and can_articles),
                )
            with b2:
                st.download_button(
                    label="í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)",
                    data=df_to_csv_bytes(df_kw_top50) if can_keywords else b"",
                    file_name=f"keywords_{base}_{ts}.csv",
                    mime="text/csv",
                    disabled=not (options["dl_keywords"] and can_keywords),
                )
            with b3:
                st.download_button(
                    label="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)",
                    data=images_zip if can_images else b"",
                    file_name=f"images_{base}_{ts}.zip",
                    mime="application/zip",
                    disabled=not (options["dl_images"] and can_images),
                )

            st.caption("â€» ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œëŠ” ì›Œë“œí´ë¼ìš°ë“œ+Top20 PNGë¥¼ ZIPìœ¼ë¡œ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.")

    # ---------------------------
    # ê¸°ì‚¬ ëª©ë¡ íƒ­
    # ---------------------------
    with tab_articles:
        st.subheader("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡")

        if df_items.empty:
            st.warning("ê¸°ì‚¬ ëª©ë¡ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        # ì •ë ¬/í•„í„° UI
        fcol1, fcol2 = st.columns([1, 2])
        with fcol1:
            sort_order = st.selectbox("ì •ë ¬", ["ìµœì‹ ìˆœ", "ì˜¤ë˜ëœìˆœ"], index=0)
        with fcol2:
            title_filter = st.text_input("ì œëª©ì— í¬í•¨ëœ ë‹¨ì–´ í•„í„°", value="")

        df_view = df_items.copy()

        # ë‚ ì§œ ì •ë ¬
        df_view["__dt"] = pd.to_datetime(df_view["pubDate"], errors="coerce")
        df_view = df_view.sort_values("__dt", ascending=(sort_order == "ì˜¤ë˜ëœìˆœ"))
        if title_filter.strip():
            df_view = df_view[df_view["title"].str.contains(title_filter, case=False, na=False)]
        df_view = df_view.drop(columns="__dt")

        st.divider()

        if not options["show_articles"]:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ 'ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°'ë¥¼ ì¼œë©´ í‘œì‹œë©ë‹ˆë‹¤.")
            return

        highlight_key = user_keyword.strip()

        MAX_SHOW = 60
        df_show = df_view.head(MAX_SHOW)

        st.caption(f"í‘œì‹œ ê¸°ì‚¬ ìˆ˜: {len(df_show)} / í•„í„°ë§ í›„ ì „ì²´: {len(df_view)}")

        for _, row in df_show.iterrows():
            title = row.get("title", "")
            pub = row.get("pubDate", "")
            link = row.get("link", "")

            title_html = highlight_keyword(title, highlight_key)

            st.markdown(
                f"""
                <div class="nk-card">
                    <div style="font-weight:800; font-size:16px; line-height:1.35;">
                        {title_html}
                    </div>
                    <div style="opacity:0.75; font-size:13px; margin-top:4px;">
                        {pub}
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )

            if options["show_links"] and link:
                st.markdown(
                    f'- <a class="nk-link" href="{link}" target="_blank">ğŸ”— ê¸°ì‚¬ ë°”ë¡œê°€ê¸°</a>',
                    unsafe_allow_html=True
                )

        if len(df_view) > MAX_SHOW:
            st.info(f"ê¸°ì‚¬ ëª©ë¡ì´ ë§ì•„ ìƒìœ„ {MAX_SHOW}ê°œë§Œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤. (í•„í„°ë¥¼ ë” ê±¸ì–´ë³´ì„¸ìš”)")

    # ---------------------------
    # í‚¤ì›Œë“œ í‘œ íƒ­
    # ---------------------------
    with tab_keywords:
        st.subheader("í‚¤ì›Œë“œ(TF-IDF) ìƒìœ„ 50")

        if df_kw_top50.empty:
            st.warning("í‚¤ì›Œë“œ í‘œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        if options["show_keywords"]:
            st.dataframe(df_kw_top50, use_container_width=True)
        else:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ 'í‚¤ì›Œë“œ í‘œ ë³´ê¸°'ë¥¼ ì¼œë©´ í‘œì‹œë©ë‹ˆë‹¤.")


# ============================================================
# 14) ì•± ì‹¤í–‰(ë©”ì¸)
# ============================================================
def run_app():
    st.set_page_config(page_title="ë‰´ìŠ¤ í‚¤ì›Œë“œ ì–´í”Œë¦¬ì¼€ì´ì…˜", layout="wide")

    inject_theme_friendly_css()
    setup_matplotlib_korean_font()

    render_header_with_lottie_and_center_title()

    render_sidebar_api_settings()

    # âœ… ì‚¬ì´ë“œë°” ìˆœì„œ: ì˜µì…˜ -> ë¶ˆìš©ì–´
    options = render_sidebar_options()
    stopwords = render_sidebar_stopwords()

    form = render_search_form()

    status_box = st.empty()
    progress_bar = st.progress(0)

    st.session_state.setdefault("result_ready", False)

    if form["submitted"]:
        clear_results_session()
        progress_bar.progress(0)
        run_pipeline(form, stopwords, status_box, progress_bar)

    render_results_tabs(options, user_keyword=form["user_keyword"])


if __name__ == "__main__":
    run_app()
