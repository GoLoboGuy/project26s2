#
# ============================================================
#  News Keyword Visualizer V4
# ------------------------------------------------------------
#  âœ… V3 ê¸°ëŠ¥ì€ ìœ ì§€í•˜ë©´ì„œ UIë§Œ ê³ ë„í™”í•œ ë²„ì „
#     (UI Improved + Safe Guards)
#
#  [UI/UX ê°œì„ ]
#  1) ê²°ê³¼ íƒ­ 3ê°œ: ìš”ì•½ / ê¸°ì‚¬ ëª©ë¡ / í‚¤ì›Œë“œ í‘œ
#  2) ìš”ì•½ íƒ­:
#     - Top í‚¤ì›Œë“œ 5ê°œ "ë°°ì§€" ìš”ì•½(í•œ ì¤„)
#     - ì›Œë“œí´ë¼ìš°ë“œ(ì¢Œ) + Top20 ë§‰ëŒ€ì°¨íŠ¸(ìš°) 2ì—´ ë°°ì¹˜
#     - ë‹¤ìš´ë¡œë“œ ì•¡ì…˜ ë°”(1ì¤„ 3ë²„íŠ¼)
#  3) ê¸°ì‚¬ ëª©ë¡ íƒ­:
#     - ì œëª© ë‚´ "ê²€ìƒ‰ì–´ í•˜ì´ë¼ì´íŠ¸" í‘œì‹œ(<mark>)
#     - ê°„ë‹¨ í•„í„°/ì •ë ¬ UI:
#         * ì •ë ¬: ìµœì‹ ìˆœ/ì˜¤ë˜ëœìˆœ
#         * ì œëª© í¬í•¨ ë‹¨ì–´ í•„í„°
#
#  [ì•ˆì •ì„±(ë°©ì–´ì½”ë“œ)]
#  - API ì¸ì¦ ì‹¤íŒ¨ ì²˜ë¦¬(401/403)
#  - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜(timeout/connection/request exception)
#  - API ì‘ë‹µ ì½”ë“œê°€ 200ì´ ì•„ë‹ˆë©´ st.error("API ìš”ì²­ ì‹¤íŒ¨")
#  - í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ skip ì²˜ë¦¬
#  - ë°ì´í„° ë¶€ì¡± ì‹œ ì‚¬ìš©ì ì•ˆë‚´ ê°•í™”(íŒ ì œê³µ)
#
#  [ë‹¤ìš´ë¡œë“œ ì •ì±…]
#  - "ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)" ë²„íŠ¼ 1ë²ˆ í´ë¦­ìœ¼ë¡œ
#     ì›Œë“œí´ë¼ìš°ë“œ + Top20 PNG ë‘ íŒŒì¼ì„ ZIPìœ¼ë¡œ ì œê³µ
#     (ë¸Œë¼ìš°ì € ì •ì±…ìƒ ê°€ì¥ ì•ˆì •ì )
#
#  ------------------------------------------------------------
#  ì‹¤í–‰:
#     streamlit run app_v4.py
# ============================================================
#


# ============================================================
# ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ
# ============================================================
import json
import re
import pickle
import html
from datetime import datetime
from email.utils import parsedate_to_datetime
from io import BytesIO
from urllib.parse import quote
import zipfile

import requests as rq
import bs4
import pandas as pd
import numpy as np

import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from PIL import Image
from wordcloud import WordCloud
from streamlit_lottie import st_lottie

from sklearn.feature_extraction.text import TfidfVectorizer
from soynlp.noun import LRNounExtractor_v2


# ============================================================
# 0) ì „ì—­ ì„¤ì •(ê²½ë¡œ/íŒŒì¼)
# ============================================================
FONT_PATH = "./resources/NanumSquareR.ttf"
STOPWORDS_PATH = "./resources/stopwords_ko.txt"
TOKENIZER_PATH = "./resources/my_tokenizer1.model"
LOTTIE_PATH = "./resources/lottie-full-movie-experience-including-music-news-video-weather-and-lots-of-entertainment.json"

MASK_BG = {
    "ì—†ìŒ": "./resources/background_0.png",
    "íƒ€ì›": "./resources/background_1.png",
    "ë§í’ì„ ": "./resources/background_2.png",
    "í•˜íŠ¸": "./resources/background_3.png",
}


# ============================================================
# 1) í…Œë§ˆ ì¹œí™” CSS (ë¼ì´íŠ¸/ë‹¤í¬ ê³µìš©)
# ============================================================
def inject_theme_friendly_css() -> None:
    """
    Streamlitì€ ë¼ì´íŠ¸/ë‹¤í¬ ëª¨ë“œê°€ ë°”ë€Œì–´ë„ ê¸°ë³¸ í…Œë§ˆ ë³€ìˆ˜ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
    ì—¬ê¸°ì„œëŠ” 'ê³¼í•œ ìƒ‰ìƒ'ì„ í”¼í•˜ê³ , ë°°ê²½/í…Œë‘ë¦¬/í…ìŠ¤íŠ¸ë¥¼ í…Œë§ˆì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ë§ì¶¥ë‹ˆë‹¤.

    í•µì‹¬:
    - ë°°ê²½/í…ìŠ¤íŠ¸ëŠ” Streamlit ê¸°ë³¸ì„ ë”°ë¥´ê³ 
    - ì¹´ë“œ/ë°°ì§€ ë“±ì—ë§Œ ì•½í•œ í…Œë‘ë¦¬/ê·¸ë¼ë°ì´ì…˜ ëŠë‚Œ ìµœì†Œ ì ìš©
    - mark(í•˜ì´ë¼ì´íŠ¸)ë„ ë‹¤í¬ëª¨ë“œì—ì„œ ëˆˆë¶€ì‹œì§€ ì•Šê²Œ ì¡°ì ˆ
    """
    st.markdown(
        """
        <style>
        /* í˜ì´ì§€ í­/ì—¬ë°±: Streamlit ê¸°ë³¸ ìœ ì§€ */

        /* 2ì¤„ íƒ€ì´í‹€ ì˜ì—­ */
        .nk-title-wrap{
            text-align:center;
            margin: 0.25rem 0 1.0rem 0;
        }
        .nk-title-main{
            font-size: 2.0rem;
            font-weight: 850;
            line-height: 1.1;
            margin: 0;
        }
        .nk-title-sub{
            font-size: 1.1rem;
            font-weight: 700;
            opacity: 0.8;
            margin-top: 0.35rem;
        }

        /* ì¹´ë“œ UI: í…Œë§ˆì— ìì—°ìŠ¤ëŸ½ê²Œ */
        .nk-card{
            border: 1px solid rgba(128,128,128,0.25);
            border-radius: 14px;
            padding: 12px 14px;
            margin: 10px 0;
            background: rgba(128,128,128,0.06);
        }
        /* ë‹¤í¬ëª¨ë“œì—ì„œ ì¹´ë“œ ë°°ê²½ì´ ë„ˆë¬´ ë°ì§€ ì•Šê²Œ(ì•½ê°„ ë” ì–´ë‘¡ê²Œ) */
        @media (prefers-color-scheme: dark) {
          .nk-card{
            background: rgba(255,255,255,0.04);
            border: 1px solid rgba(255,255,255,0.18);
          }
        }

        /* ë°°ì§€ UI */
        .nk-badge{
            display:inline-block;
            padding:6px 12px;
            margin:4px 6px 4px 0;
            border-radius: 16px;
            font-size: 0.92rem;
            font-weight: 750;
            border: 1px solid rgba(128,128,128,0.25);
            background: rgba(99,102,241,0.10); /* ì¸ë””ê³  ê³„ì—´ ì•„ì£¼ ì—°í•˜ê²Œ */
        }
        @media (prefers-color-scheme: dark) {
          .nk-badge{
            background: rgba(99,102,241,0.18);
            border: 1px solid rgba(255,255,255,0.18);
          }
        }

        /* ì œëª© í•˜ì´ë¼ì´íŠ¸ <mark> ìŠ¤íƒ€ì¼: ë¼ì´íŠ¸/ë‹¤í¬ ê³µìš© */
        mark{
            padding: 0.08em 0.18em;
            border-radius: 0.25em;
            background: rgba(245, 158, 11, 0.35); /* amber íˆ¬ëª… */
            color: inherit;
        }
        @media (prefers-color-scheme: dark) {
          mark{
            background: rgba(245, 158, 11, 0.28);
          }
        }

        /* ë§í¬ê°€ ë„ˆë¬´ íŠ€ì§€ ì•Šê²Œ */
        .nk-link{
            opacity: 0.92;
            font-weight: 650;
        }
        </style>
        """,
        unsafe_allow_html=True
    )


# ============================================================
# 2) matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
# ============================================================
def setup_matplotlib_korean_font() -> None:
    """matplotlib í•œê¸€ ê¹¨ì§ ë°©ì§€ ì„¤ì •."""
    try:
        fm.fontManager.addfont(FONT_PATH)
        plt.rcParams["font.family"] = fm.FontProperties(fname=FONT_PATH).get_name()
    except Exception:
        plt.rcParams["font.family"] = "Malgun Gothic"
    plt.rcParams["axes.unicode_minus"] = False


# ============================================================
# 3) ë¦¬ì†ŒìŠ¤ ë¡œë”©(ìºì‹œ)
# ============================================================
def load_json(path: str) -> dict:
    """JSON íŒŒì¼ ì•ˆì „ ë¡œë“œ(ì‹¤íŒ¨ ì‹œ ë¹ˆ dict)."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


@st.cache_data(show_spinner=False)
def load_stopwords_file(path: str) -> set[str]:
    """ë¶ˆìš©ì–´ íŒŒì¼ -> set (ì‹¤íŒ¨ ì‹œ ë¹ˆ set)."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return {w.strip() for w in f if w.strip()}
    except Exception:
        return set()


@st.cache_resource
def load_tokenizer():
    """í† í¬ë‚˜ì´ì € ë¡œë“œ(ì‹¤íŒ¨ ì‹œ None)."""
    try:
        with open(TOKENIZER_PATH, "rb") as f:
            return pickle.load(f)
    except Exception:
        return None


# ============================================================
# 4) í…ìŠ¤íŠ¸ ìœ í‹¸
# ============================================================
def clean_title(raw_title: str) -> str:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ titleì˜ HTML ì œê±° + ê³µë°± ì •ë¦¬."""
    t = html.unescape(raw_title or "")
    t = re.sub(r"<.*?>", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def format_pubdate(pub_date: str) -> str:
    """pubDateë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¬¸ìì—´ë¡œ ë³€í™˜."""
    try:
        dt = parsedate_to_datetime(pub_date)
        return dt.strftime("%Y-%m-%d %H:%M")
    except Exception:
        return pub_date or ""


@st.cache_data(show_spinner=False)
def clean_text_keep_korean(text: str) -> str:
    """ìˆ«ì/ì˜ë¬¸/íŠ¹ìˆ˜ë¬¸ì ì œê±° + ê³µë°± ì •ë¦¬(í•œê¸€ ì¤‘ì‹¬)."""
    text = re.sub(r"\d|[a-zA-Z]|\W", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def normalize_token(t: str) -> str:
    """í† í° ì •ê·œí™”(êµ¬ë‘ì /ê³µë°± ì œê±°)."""
    if t is None:
        return ""
    t = str(t).strip()
    t = re.sub(r"[\"'â€œâ€â€˜â€™\(\)\[\]\{\},\.\!\?\:\;]", "", t)
    t = re.sub(r"\s+", "", t)
    return t


def build_final_keyword(category: str, user_keyword: str) -> str:
    """ë¶„ì•¼ + ì‚¬ìš©ì í‚¤ì›Œë“œ ê²°í•©(ê³µë°± ê¸°ë°˜)."""
    category = (category or "").strip()
    user_keyword = re.sub(r"\s+", " ", (user_keyword or "")).strip()
    return f"{category} {user_keyword}".strip()


def safe_filename(s: str) -> str:
    """íŒŒì¼ëª… ì•ˆì „í™”."""
    s = s.strip()
    s = re.sub(r"[^\w\-ê°€-í£]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "result"


def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    """CSV ë‹¤ìš´ë¡œë“œìš© bytes(utf-8-sig)."""
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


def highlight_keyword(text: str, keyword: str) -> str:
    """ì œëª© ë‚´ í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸(<mark>)."""
    if not keyword:
        return text
    try:
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        return pattern.sub(lambda m: f"<mark>{m.group(0)}</mark>", text)
    except Exception:
        return text


# ============================================================
# 5) ë„¤ì´ë²„ API(ë°©ì–´ ì½”ë“œ)
# ============================================================
def naver_news_api_request(keyword: str, display: int, start: int, client_id: str, client_secret: str) -> list[dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API(1í˜ì´ì§€).
    - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°©ì–´
    - ì¸ì¦ ì‹¤íŒ¨/HTTP ì˜¤ë¥˜ ë°©ì–´
    - status_code != 200ì´ë©´ st.error("API ìš”ì²­ ì‹¤íŒ¨")
    """
    if not client_id.strip() or not client_secret.strip():
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return []

    url = f"https://openapi.naver.com/v1/search/news.json?query={quote(keyword)}&display={display}&start={start}"
    headers = {
        "X-Naver-Client-Id": client_id.strip(),
        "X-Naver-Client-Secret": client_secret.strip(),
    }

    try:
        res = rq.get(url, headers=headers, timeout=10)
    except rq.exceptions.Timeout:
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤(timeout).")
        return []
    except rq.exceptions.ConnectionError:
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ConnectionError).")
        return []
    except rq.exceptions.RequestException as e:
        st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return []

    # âœ… ìš”êµ¬ì‚¬í•­
    if res.status_code != 200:
        st.error("API ìš”ì²­ ì‹¤íŒ¨")
        if res.status_code in (401, 403):
            st.warning("API ì¸ì¦ ì‹¤íŒ¨(ê¶Œí•œ/í‚¤ ì˜¤ë¥˜). Client ID/Secretì„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            st.warning(f"HTTP ìƒíƒœì½”ë“œ: {res.status_code}")
        return []

    try:
        data = res.json()
        return data.get("items", []) or []
    except Exception:
        st.error("API ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨")
        return []


def fetch_news_items(final_keyword: str, total_display: int, client_id: str, client_secret: str) -> list[dict]:
    """100ë‹¨ìœ„ë¡œ í˜ì´ì§€ ìš”ì²­ í›„ items í•©ì¹˜ê¸°(ì¼ë¶€ ì‹¤íŒ¨í•´ë„ ê³„ì†)."""
    items: list[dict] = []
    page_count = max(1, total_display // 100)

    for i in range(page_count):
        start = 100 * i + 1
        page_items = naver_news_api_request(final_keyword, 100, start, client_id, client_secret)
        if page_items:
            items.extend(page_items)

    return items


def build_items_dataframe(items: list[dict]) -> pd.DataFrame:
    """itemsì—ì„œ title/pubDate/linkë§Œ ì¶”ì¶œ."""
    rows = []
    for it in items:
        rows.append({
            "title": clean_title(it.get("title", "")),
            "pubDate": format_pubdate(it.get("pubDate", "")),
            "link": it.get("link", ""),
        })
    return pd.DataFrame(rows)


# ============================================================
# 6) í¬ë¡¤ë§(ì‹¤íŒ¨ skip)
# ============================================================
def crawl_naver_news_body(url: str) -> str:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§(ì‹¤íŒ¨ ì‹œ '' ë°˜í™˜)."""
    try:
        res = rq.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        if res.status_code != 200:
            return ""
        soup = bs4.BeautifulSoup(res.text, "html.parser")
        tag = soup.select_one("#dic_area")
        return tag.get_text(separator=" ", strip=True) if tag else ""
    except Exception:
        return ""


def collect_corpus_from_items(items: list[dict]) -> list[str]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ë§Œ ë³¸ë¬¸ ìˆ˜ì§‘. ì‹¤íŒ¨/ì§§ì€ ë³¸ë¬¸ skip."""
    docs = []
    for it in items:
        link = it.get("link", "")
        if "n.news.naver" not in link:
            continue

        body = crawl_naver_news_body(link)
        if not body:
            continue

        cleaned = clean_text_keep_korean(body)
        if len(cleaned) < 100:
            continue

        docs.append(cleaned)

    return docs


# ============================================================
# 7) ë¶„ì„(soynlp ëª…ì‚¬ set + TF-IDF)
# ============================================================
@st.cache_data(show_spinner=False)
def build_noun_set(docs_clean: list[str]) -> set[str]:
    """soynlpë¡œ ëª…ì‚¬ í›„ë³´ set ìƒì„±(ë°ì´í„° ì ìœ¼ë©´ ë¹ˆ set)."""
    sents = []
    for d in docs_clean:
        sents.extend([s.strip() for s in re.split(r"[\.!?]\s*|\n", d) if len(s.strip()) >= 10])

    if len(sents) < 5:
        return set()

    extractor = LRNounExtractor_v2(verbose=False)
    extractor.train(sents)
    nouns = extractor.extract()

    MIN_FREQ = 2
    MIN_SCORE = 0.4

    noun_set = set()
    for w, score in nouns.items():
        freq = getattr(score, "frequency", None)
        sc = getattr(score, "score", None)

        if freq is None and isinstance(score, dict):
            freq = score.get("frequency")
        if sc is None and isinstance(score, dict):
            sc = score.get("score")

        if freq is not None and freq < MIN_FREQ:
            continue
        if sc is not None and sc < MIN_SCORE:
            continue
        noun_set.add(w)

    return noun_set


def tokenize_and_filter_docs(docs_clean: list[str], stopwords: set[str]) -> list[list[str]]:
    """
    í† í°í™” -> ëª…ì‚¬ í•„í„°(ê°€ëŠ¥í•˜ë©´) -> ë¶ˆìš©ì–´ ì œê±°
    - í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ ì‹œ split fallback
    - ëª…ì‚¬ setì´ ë¹„ë©´(ë°ì´í„° ë¶€ì¡±) ëª…ì‚¬ í•„í„° ì™„í™”
    """
    tokenizer = load_tokenizer()
    noun_set = build_noun_set(docs_clean)

    docs_tokens: list[list[str]] = []

    for d in docs_clean:
        if tokenizer is None:
            raw_tokens = d.split()
        else:
            try:
                raw_tokens = [t1 for t1, _ in tokenizer.tokenize(d, flatten=False)]
            except Exception:
                raw_tokens = d.split()

        filtered = []
        for t in raw_tokens:
            nt = normalize_token(t)
            if not nt:
                continue
            if not (2 <= len(nt) <= 8):
                continue
            if nt in stopwords:
                continue
            if noun_set and (nt not in noun_set):
                continue
            filtered.append(nt)

        docs_tokens.append(filtered)

    return docs_tokens


def compute_tfidf_scores(docs_tokens: list[list[str]], top_k: int = 80) -> dict[str, float]:
    """TF-IDF ì ìˆ˜ ê³„ì‚°(ì˜¤ë¥˜/ë¶€ì¡± ì‹œ ë¹ˆ dict)."""
    docs_str = [" ".join(ts) for ts in docs_tokens if ts]
    if len(docs_str) < 2:
        return {}

    try:
        vec = TfidfVectorizer(
            tokenizer=str.split,
            token_pattern=None,
            lowercase=False,
            min_df=2,
        )
        X = vec.fit_transform(docs_str)
        terms = np.array(vec.get_feature_names_out())
        scores = np.asarray(X.sum(axis=0)).ravel()

        if scores.size == 0:
            return {}

        idx = np.argsort(scores)[::-1][:top_k]
        return {terms[i]: float(scores[i]) for i in idx}
    except Exception:
        return {}


def build_keyword_tables(score_dict: dict[str, float]) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """ì ìˆ˜ dict -> DataFrame + Top50/Top20."""
    df_kw = (
        pd.DataFrame(list(score_dict.items()), columns=["keyword", "score"])
        .sort_values("score", ascending=False)
    )
    return df_kw, df_kw.head(50).copy(), df_kw.head(20).copy()


# ============================================================
# 8) ì‹œê°í™”(ì´ë¯¸ì§€ bytes)
# ============================================================
def fig_to_png_bytes(fig) -> bytes:
    """matplotlib figure -> PNG bytes."""
    buf = BytesIO()
    fig.savefig(buf, format="png", dpi=160, bbox_inches="tight")
    buf.seek(0)
    return buf.getvalue()


def make_wordcloud_png(freq: dict[str, float], mask_name: str) -> bytes | None:
    """ì›Œë“œí´ë¼ìš°ë“œ PNG bytes ìƒì„±."""
    if not freq:
        return None

    bg_path = MASK_BG.get(mask_name, MASK_BG["ì—†ìŒ"])
    mask = None
    try:
        img = Image.open(bg_path)
        mask = np.array(img)
    except Exception:
        mask = None

    wc = WordCloud(
        font_path=FONT_PATH,
        background_color="white",
        max_words=80,
        mask=mask,
    ).generate_from_frequencies(freq)

    fig = plt.figure(figsize=(10, 10))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")

    png = fig_to_png_bytes(fig)
    plt.close(fig)
    return png


def make_top20_bar_png(df_top20: pd.DataFrame) -> bytes | None:
    """Top20 ë§‰ëŒ€ì°¨íŠ¸ PNG bytes ìƒì„±."""
    if df_top20.empty:
        return None

    fig = plt.figure(figsize=(10, 5))
    plt.bar(df_top20["keyword"], df_top20["score"])
    plt.xticks(rotation=45, ha="right")
    plt.title("TF-IDF ìƒìœ„ í‚¤ì›Œë“œ (Top 20)")
    plt.tight_layout()

    png = fig_to_png_bytes(fig)
    plt.close(fig)
    return png


def make_images_zip_bytes(wordcloud_png: bytes, top20_png: bytes, base_name: str) -> bytes:
    """ì›Œë“œí´ë¼ìš°ë“œ + Top20 ì´ë¯¸ì§€ë¥¼ ZIPìœ¼ë¡œ ë¬¶ì–´ì„œ ë°˜í™˜."""
    zip_buf = BytesIO()
    with zipfile.ZipFile(zip_buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr(f"{base_name}_wordcloud.png", wordcloud_png)
        zf.writestr(f"{base_name}_top20.png", top20_png)
    zip_buf.seek(0)
    return zip_buf.getvalue()


# ============================================================
# 9) UI ë Œë”ë§
# ============================================================
def render_header_with_lottie_and_center_title():
    """
    âœ… LottieëŠ” ê¸°ì¡´ì²˜ëŸ¼ ì¢Œì¸¡ì— ë°°ì¹˜
    âœ… íƒ€ì´í‹€ì€ 2ì¤„ë¡œ ê°€ìš´ë° ì •ë ¬
    """
    col1, col2 = st.columns([1, 2.2])

    with col1:
        lottie = load_json(LOTTIE_PATH)
        if lottie:
            st_lottie(lottie, speed=1, loop=True, width=200, height=200)

    with col2:
        # íƒ€ì´í‹€ì€ HTMLë¡œ ê°€ìš´ë° ì •ë ¬ + 2ì¤„ í‘œí˜„
        st.markdown(
            """
            <div class="nk-title-wrap">
                <div class="nk-title-main">ë‰´ìŠ¤ í‚¤ì›Œë“œ ì–´í”Œë¦¬ì¼€ì´ì…˜</div>
                <div class="nk-title-sub">(ë¶„ì„ &amp; ì‹œê°í™”)</div>
            </div>
            """,
            unsafe_allow_html=True
        )


def render_sidebar_api_settings():
    """ì‚¬ì´ë“œë°” API ì„¤ì • í¼."""
    st.sidebar.header("API Keys :")
    st.session_state.setdefault("client_id", "")
    st.session_state.setdefault("client_secret", "")

    with st.sidebar.form("client_settings", clear_on_submit=False):
        cid = st.text_input("Client ID:", value=st.session_state["client_id"])
        secret = st.text_input("Client Secret:", type="password", value=st.session_state["client_secret"])
        if st.form_submit_button("OK"):
            st.session_state["client_id"] = (cid or "").strip()
            st.session_state["client_secret"] = (secret or "").strip()
            st.rerun()


def render_sidebar_options():
    """
    âœ… ì˜µì…˜ ì²´í¬ë°•ìŠ¤
    - 1ì¤„: ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°, ë§í¬ ì œê³µ, ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)
    - 2ì¤„: í‚¤ì›Œë“œ í‘œ ë³´ê¸°, í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv), ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)
    """
    st.sidebar.header("í‘œì‹œ/ë‹¤ìš´ë¡œë“œ ì˜µì…˜ :")
    r1c1, r1c2, r1c3 = st.sidebar.columns(3)
    with r1c1:
        show_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°", value=True, key="opt_show_articles")
    with r1c2:
        show_links = st.checkbox("ë§í¬ ì œê³µ", value=False, key="opt_show_links")
    with r1c3:
        dl_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)", value=False, key="opt_dl_articles")

    r2c1, r2c2, r2c3 = st.sidebar.columns(3)
    with r2c1:
        show_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë³´ê¸°", value=True, key="opt_show_keywords")
    with r2c2:
        dl_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)", value=False, key="opt_dl_keywords")
    with r2c3:
        dl_images = st.checkbox("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)", value=False, key="opt_dl_images")

    return {
        "show_articles": show_articles,
        "show_links": show_links,
        "dl_articles": dl_articles,
        "show_keywords": show_keywords,
        "dl_keywords": dl_keywords,
        "dl_images": dl_images,
    }


def render_sidebar_stopwords() -> set[str]:
    """
    âœ… ë¶ˆìš©ì–´ ì˜ì—­
    - íŒŒì¼ ë¶ˆìš©ì–´ + ì¶”ê°€ ì…ë ¥ ë¶ˆìš©ì–´ë¥¼ í•©ì³ì„œ ë°˜í™˜
    """
    st.sidebar.header("ë¶ˆìš©ì–´(Stopwords) :")
    base_stop = load_stopwords_file(STOPWORDS_PATH)
    extra_stop = st.sidebar.text_area("ì¶”ê°€ ë¶ˆìš©ì–´(ì¤„ë°”ê¿ˆìœ¼ë¡œ ì…ë ¥)", value="", height=120)
    stopwords = base_stop | {w.strip() for w in extra_stop.splitlines() if w.strip()}
    st.sidebar.caption(f"í˜„ì¬ ë¶ˆìš©ì–´ ìˆ˜: {len(stopwords)} (íŒŒì¼ + ì¶”ê°€ ì…ë ¥)")
    return stopwords


def render_search_form():
    """
    ë©”ì¸ ì…ë ¥ í¼(UI ì¹´ë“œ)
    - ë¶„ì•¼/í‚¤ì›Œë“œ/ë¶„ëŸ‰ 3ì—´
    - ë°±ë§ˆìŠ¤í¬ ë¼ë””ì˜¤
    """
    with st.container(border=True):
        st.subheader("ê²€ìƒ‰ ì¡°ê±´")

        with st.form("search", clear_on_submit=False):
            c1, c2, c3 = st.columns([1, 2, 1])

            with c1:
                category = st.selectbox("ë¶„ì•¼", ["ê²½ì œ", "ì •ì¹˜", "ì‚¬íšŒ", "êµ­ì œ", "ì—°ì˜ˆ", "IT", "ë¬¸í™”"])
            with c2:
                user_keyword = st.text_input(
                    "ê²€ìƒ‰ í‚¤ì›Œë“œ(í•„ìˆ˜)",
                    value="",
                    placeholder="ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI, ë©”íƒ€ë²„ìŠ¤ ..."
                )
            with c3:
                display = st.select_slider("ë¶„ëŸ‰", options=[100, 200, 300, 400, 500], value=100)

            mask = st.radio("ë°±ë§ˆìŠ¤í¬", ["ì—†ìŒ", "íƒ€ì›", "ë§í’ì„ ", "í•˜íŠ¸"], horizontal=True)
            submit = st.form_submit_button("ê²€ìƒ‰ ì‹¤í–‰")

    return {
        "category": category,
        "user_keyword": user_keyword,
        "display": display,
        "mask": mask,
        "submitted": submit,
    }


# ============================================================
# 10) ê²°ê³¼ ì„¸ì…˜ ì €ì¥/ì´ˆê¸°í™”
# ============================================================
def save_results_to_session(
    final_keyword: str,
    df_items: pd.DataFrame,
    df_kw_top50: pd.DataFrame,
    df_kw_top20: pd.DataFrame,
    wc_png: bytes,
    top20_png: bytes,
    zip_bytes: bytes,
):
    st.session_state["result_ready"] = True
    st.session_state["final_keyword"] = final_keyword
    st.session_state["df_items"] = df_items
    st.session_state["df_kw_top50"] = df_kw_top50
    st.session_state["df_kw_top20"] = df_kw_top20
    st.session_state["wc_png"] = wc_png
    st.session_state["top20_png"] = top20_png
    st.session_state["images_zip"] = zip_bytes


def clear_results_session():
    st.session_state["result_ready"] = False
    for k in ["final_keyword", "df_items", "df_kw_top50", "df_kw_top20", "wc_png", "top20_png", "images_zip"]:
        if k in st.session_state:
            del st.session_state[k]


# ============================================================
# 11) íŒŒì´í”„ë¼ì¸ ì‹¤í–‰(ìƒíƒœë°•ìŠ¤+ì§„í–‰ë°”)
# ============================================================
def run_pipeline(form: dict, stopwords: set[str], status_box, progress_bar):
    """
    íŒŒì´í”„ë¼ì¸ ì‹¤í–‰:
    1) API ìˆ˜ì§‘
    2) í¬ë¡¤ë§
    3) ë¶„ì„
    4) ì‹œê°í™”
    """
    if not form["user_keyword"].strip():
        st.warning("ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI)")
        return

    client_id = st.session_state.get("client_id", "").strip()
    client_secret = st.session_state.get("client_secret", "").strip()
    if not client_id or not client_secret:
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì…ë ¥ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return

    final_keyword = build_final_keyword(form["category"], form["user_keyword"])

    # 1) API ìˆ˜ì§‘
    status_box.info(f"1/4 ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ ì¤‘... (ê²€ìƒ‰ì–´: {final_keyword})")
    progress_bar.progress(0.2)
    items = fetch_news_items(final_keyword, form["display"], client_id, client_secret)

    if not items:
        status_box.error("ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.info("ê°€ëŠ¥í•œ ì›ì¸: (1) ì¸ì¦ ì‹¤íŒ¨ (2) ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (3) ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return

    df_items = build_items_dataframe(items)

    # 2) í¬ë¡¤ë§
    status_box.info("2/4 ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
    progress_bar.progress(0.45)
    docs_clean = collect_corpus_from_items(items)

    if len(docs_clean) < 5:
        status_box.warning("ë³¸ë¬¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.")
        st.info(
            "ê°œì„  íŒ:\n"
            "- ë¶„ëŸ‰ì„ 300~500ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”.\n"
            "- í‚¤ì›Œë“œë¥¼ ë” ì¼ë°˜ì ìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”.\n"
            "- ê¸°ì‚¬ ëª©ë¡ì—ì„œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•´ë³´ì„¸ìš”."
        )
        return

    # 3) ë¶„ì„
    status_box.info("3/4 í‚¤ì›Œë“œ ë¶„ì„ ì¤‘(ëª…ì‚¬ í•„í„° + TF-IDF)...")
    progress_bar.progress(0.7)
    docs_tokens = tokenize_and_filter_docs(docs_clean, stopwords)

    score_dict = compute_tfidf_scores(docs_tokens, top_k=80)
    if not score_dict:
        status_box.warning("í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ë°ì´í„°/í•„í„° ì¡°ê±´ ë¶€ì¡±).")
        st.info("ê°œì„  íŒ: ë¶„ëŸ‰ì„ ëŠ˜ë¦¬ê±°ë‚˜ ë¶ˆìš©ì–´ë¥¼ ê³¼ë„í•˜ê²Œ ì¶”ê°€í•˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return

    _, df_kw_top50, df_kw_top20 = build_keyword_tables(score_dict)

    # 4) ì‹œê°í™” ìƒì„± (PNG bytes)
    status_box.info("4/4 ì‹œê°í™” ìƒì„± ì¤‘...")
    progress_bar.progress(0.9)

    wc_png = make_wordcloud_png(score_dict, form["mask"])
    top20_png = make_top20_bar_png(df_kw_top20)

    if not wc_png or not top20_png:
        status_box.error("ì‹œê°í™” ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤(ë°ì´í„° ë¶€ì¡±/ë Œë”ë§ ì˜¤ë¥˜).")
        return

    base = safe_filename(final_keyword)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_bytes = make_images_zip_bytes(wc_png, top20_png, f"{base}_{ts}")

    progress_bar.progress(1.0)
    status_box.success("ì™„ë£Œ! ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    save_results_to_session(
        final_keyword=final_keyword,
        df_items=df_items,
        df_kw_top50=df_kw_top50,
        df_kw_top20=df_kw_top20,
        wc_png=wc_png,
        top20_png=top20_png,
        zip_bytes=zip_bytes,
    )


# ============================================================
# 12) ê²°ê³¼ íƒ­ UI
# ============================================================
def render_top5_badges(df_kw_top50: pd.DataFrame) -> None:
    """ìš”ì•½ íƒ­ì— Top5 í‚¤ì›Œë“œë¥¼ ë°°ì§€ë¡œ ë Œë”ë§."""
    top5 = df_kw_top50.head(5)["keyword"].tolist()

    badges = "".join([f'<span class="nk-badge">#{kw}</span>' for kw in top5])

    st.markdown(
        f"""
        <div style="margin:6px 0 14px 0;">
            <div style="font-weight:800; margin-bottom:6px;">í•µì‹¬ í‚¤ì›Œë“œ ìš”ì•½</div>
            <div>{badges}</div>
        </div>
        """,
        unsafe_allow_html=True
    )


def render_results_tabs(options: dict, user_keyword: str) -> None:
    """
    íƒ­ ê¸°ë°˜ ê²°ê³¼ UI:
    - ìš”ì•½ / ê¸°ì‚¬ ëª©ë¡ / í‚¤ì›Œë“œ í‘œ
    """
    if not st.session_state.get("result_ready", False):
        st.info("ê²€ìƒ‰ ì‹¤í–‰ í›„ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")
        return

    final_keyword = st.session_state["final_keyword"]
    df_items: pd.DataFrame = st.session_state["df_items"]
    df_kw_top50: pd.DataFrame = st.session_state["df_kw_top50"]
    df_kw_top20: pd.DataFrame = st.session_state["df_kw_top20"]

    wc_png: bytes = st.session_state["wc_png"]
    top20_png: bytes = st.session_state["top20_png"]
    images_zip: bytes = st.session_state["images_zip"]

    tab_summary, tab_articles, tab_keywords = st.tabs(["ìš”ì•½", "ê¸°ì‚¬ ëª©ë¡", "í‚¤ì›Œë“œ í‘œ"])

    # ---------------------------
    # ìš”ì•½ íƒ­
    # ---------------------------
    with tab_summary:
        st.subheader(f"ë¶„ì„ ìš”ì•½: {final_keyword}")

        if not df_kw_top50.empty:
            render_top5_badges(df_kw_top50)

        left, right = st.columns(2)
        with left:
            st.caption("ì›Œë“œí´ë¼ìš°ë“œ")
            st.image(wc_png, use_container_width=True)
        with right:
            st.caption("Top20 ë§‰ëŒ€ì°¨íŠ¸")
            st.image(top20_png, use_container_width=True)

        # ë‹¤ìš´ë¡œë“œ ì•¡ì…˜ë°”(1ì¤„ 3ë²„íŠ¼)
        with st.container(border=True):
            st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")

            can_articles = not df_items.empty
            can_keywords = not df_kw_top50.empty
            can_images = bool(images_zip)

            base = safe_filename(final_keyword)
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")

            b1, b2, b3 = st.columns(3)
            with b1:
                st.download_button(
                    label="ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)",
                    data=df_to_csv_bytes(df_items) if can_articles else b"",
                    file_name=f"articles_{base}_{ts}.csv",
                    mime="text/csv",
                    disabled=not (options["dl_articles"] and can_articles),
                )
            with b2:
                st.download_button(
                    label="í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)",
                    data=df_to_csv_bytes(df_kw_top50) if can_keywords else b"",
                    file_name=f"keywords_{base}_{ts}.csv",
                    mime="text/csv",
                    disabled=not (options["dl_keywords"] and can_keywords),
                )
            with b3:
                st.download_button(
                    label="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)",
                    data=images_zip if can_images else b"",
                    file_name=f"images_{base}_{ts}.zip",
                    mime="application/zip",
                    disabled=not (options["dl_images"] and can_images),
                )

            st.caption("â€» ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œëŠ” ì›Œë“œí´ë¼ìš°ë“œ+Top20 PNGë¥¼ ZIPìœ¼ë¡œ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.")

    # ---------------------------
    # ê¸°ì‚¬ ëª©ë¡ íƒ­
    # ---------------------------
    with tab_articles:
        st.subheader("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡")

        if df_items.empty:
            st.warning("ê¸°ì‚¬ ëª©ë¡ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        # ì •ë ¬/í•„í„° UI
        fcol1, fcol2 = st.columns([1, 2])
        with fcol1:
            sort_order = st.selectbox("ì •ë ¬", ["ìµœì‹ ìˆœ", "ì˜¤ë˜ëœìˆœ"], index=0)
        with fcol2:
            title_filter = st.text_input("ì œëª©ì— í¬í•¨ëœ ë‹¨ì–´ í•„í„°", value="")

        df_view = df_items.copy()

        # ë‚ ì§œ ì •ë ¬
        df_view["__dt"] = pd.to_datetime(df_view["pubDate"], errors="coerce")
        df_view = df_view.sort_values("__dt", ascending=(sort_order == "ì˜¤ë˜ëœìˆœ"))
        if title_filter.strip():
            df_view = df_view[df_view["title"].str.contains(title_filter, case=False, na=False)]
        df_view = df_view.drop(columns="__dt")

        st.divider()

        if not options["show_articles"]:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ 'ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°'ë¥¼ ì¼œë©´ í‘œì‹œë©ë‹ˆë‹¤.")
            return

        highlight_key = user_keyword.strip()  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ì›Œë“œë¥¼ í•˜ì´ë¼ì´íŠ¸ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©

        MAX_SHOW = 60
        df_show = df_view.head(MAX_SHOW)

        st.caption(f"í‘œì‹œ ê¸°ì‚¬ ìˆ˜: {len(df_show)} / í•„í„°ë§ í›„ ì „ì²´: {len(df_view)}")

        for _, row in df_show.iterrows():
            title = row.get("title", "")
            pub = row.get("pubDate", "")
            link = row.get("link", "")

            title_html = highlight_keyword(title, highlight_key)

            st.markdown(
                f"""
                <div class="nk-card">
                    <div style="font-weight:800; font-size:16px; line-height:1.35;">
                        {title_html}
                    </div>
                    <div style="opacity:0.75; font-size:13px; margin-top:4px;">
                        {pub}
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )

            if options["show_links"] and link:
                st.markdown(f'- <a class="nk-link" href="{link}" target="_blank">ğŸ”— ê¸°ì‚¬ ë°”ë¡œê°€ê¸°</a>', unsafe_allow_html=True)

        if len(df_view) > MAX_SHOW:
            st.info(f"ê¸°ì‚¬ ëª©ë¡ì´ ë§ì•„ ìƒìœ„ {MAX_SHOW}ê°œë§Œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤. (í•„í„°ë¥¼ ë” ê±¸ì–´ë³´ì„¸ìš”)")

    # ---------------------------
    # í‚¤ì›Œë“œ í‘œ íƒ­
    # ---------------------------
    with tab_keywords:
        st.subheader("í‚¤ì›Œë“œ(TF-IDF) ìƒìœ„ 50")

        if df_kw_top50.empty:
            st.warning("í‚¤ì›Œë“œ í‘œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        if options["show_keywords"]:
            st.dataframe(df_kw_top50, use_container_width=True)
        else:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ 'í‚¤ì›Œë“œ í‘œ ë³´ê¸°'ë¥¼ ì¼œë©´ í‘œì‹œë©ë‹ˆë‹¤.")


# ============================================================
# 13) ì•± ì‹¤í–‰(ë©”ì¸)
# ============================================================
def run_app():
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(page_title="ë‰´ìŠ¤ í‚¤ì›Œë“œ ì–´í”Œë¦¬ì¼€ì´ì…˜", layout="wide")

    # í…Œë§ˆ ì¹œí™” CSS ì£¼ì… (ë¼ì´íŠ¸/ë‹¤í¬ ìì—°ìŠ¤ëŸ¬ìš´ ìƒ‰ê°)
    inject_theme_friendly_css()

    # matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
    setup_matplotlib_korean_font()

    # í—¤ë”(Lottie + 2ì¤„ ê°€ìš´ë° íƒ€ì´í‹€)
    render_header_with_lottie_and_center_title()

    # ì‚¬ì´ë“œë°” API ì„¤ì •
    render_sidebar_api_settings()

    # âœ… ì‚¬ì´ë“œë°” ìœ„ì¹˜ ë³€ê²½: ì˜µì…˜ -> ë¶ˆìš©ì–´
    options = render_sidebar_options()
    stopwords = render_sidebar_stopwords()

    # ë©”ì¸ ì…ë ¥ í¼
    form = render_search_form()

    # ìƒíƒœ UI: í•œ ê°œë§Œ ì—…ë°ì´íŠ¸ë˜ë„ë¡ empty + progress ì‚¬ìš©
    status_box = st.empty()
    progress_bar = st.progress(0)

    # ì„¸ì…˜ ì´ˆê¸°í™”
    st.session_state.setdefault("result_ready", False)

    # ê²€ìƒ‰ ì‹¤í–‰ ì‹œ: ì´ì „ ê²°ê³¼ ì œê±° í›„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    if form["submitted"]:
        clear_results_session()
        progress_bar.progress(0)
        run_pipeline(form, stopwords, status_box, progress_bar)

    # ê²°ê³¼ íƒ­ ë Œë”ë§ (user_keyword ì „ë‹¬: í•˜ì´ë¼ì´íŠ¸ ìš©)
    render_results_tabs(options, user_keyword=form["user_keyword"])


if __name__ == "__main__":
    run_app()
