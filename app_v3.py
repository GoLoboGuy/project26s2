#
# ===================================================
# News Keyword Visualizer V3
# ---------------------------------------------------
#
# - ì—­í• ë³„ í•¨ìˆ˜ ë¶„ë¦¬ 
#   (API / í¬ë¡¤ë§ / ì „ì²˜ë¦¬ / ë¶„ì„ / ì‹œê°í™” / ë‹¤ìš´ë¡œë“œ / UI)
#
# - ì˜ˆì™¸ ìƒí™© ë°©ì–´ ê°•í™” (ì•±ì´ ì£½ì§€ ì•Šë„ë¡ ì²˜ë¦¬)
#   * API ì¸ì¦ ì‹¤íŒ¨ ì²˜ë¦¬(401/403 ë“±)
#   * ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì²˜ë¦¬(timeout, connection error ë“±)
#   * í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ skip ì²˜ë¦¬
#   * ë°ì´í„° ë¶€ì¡± ì‹œ ì‚¬ìš©ì ì•ˆë‚´ ê°•í™”
#    if res.status_code != 200: st.error("API ìš”ì²­ ì‹¤íŒ¨")
#
# ===================================================
#

import json
import re
import pickle
import html
from datetime import datetime
from email.utils import parsedate_to_datetime
from io import BytesIO
from urllib.parse import quote
import zipfile

import requests as rq
import bs4
import pandas as pd
import numpy as np

import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from PIL import Image
from wordcloud import WordCloud
from streamlit_lottie import st_lottie

from sklearn.feature_extraction.text import TfidfVectorizer
from soynlp.noun import LRNounExtractor_v2


# ===================================================
# 0) ì „ì—­ ì„¤ì •(í°íŠ¸, ê²½ë¡œ)
# ===================================================
FONT_PATH = "./resources/NanumSquareR.ttf"
STOPWORDS_PATH = "./resources/stopwords_ko.txt"
TOKENIZER_PATH = "./resources/my_tokenizer1.model"

LOTTIE_PATH = "./resources/lottie-full-movie-experience-including-music-news-video-weather-and-lots-of-entertainment.json"

MASK_BG = {
    "ì—†ìŒ": "./resources/background_0.png",
    "íƒ€ì›": "./resources/background_1.png",
    "ë§í’ì„ ": "./resources/background_2.png",
    "í•˜íŠ¸": "./resources/background_3.png",
}


def setup_matplotlib_korean_font() -> None:
    """
    ë§‰ëŒ€ì°¨íŠ¸ ë“± matplotlib ì¶œë ¥ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê²Œ í°íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    í°íŠ¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìœˆë„ìš° ê¸°ë³¸ í°íŠ¸(Malgun Gothic)ë¡œ fallback í•©ë‹ˆë‹¤.
    """
    try:
        fm.fontManager.addfont(FONT_PATH)
        plt.rcParams["font.family"] = fm.FontProperties(fname=FONT_PATH).get_name()
    except Exception:
        plt.rcParams["font.family"] = "Malgun Gothic"
    plt.rcParams["axes.unicode_minus"] = False


# ===================================================
# 1) ë¡œë”©/ìºì‹œ í•¨ìˆ˜ë“¤(ë¦¬ì†ŒìŠ¤)
# ===================================================
def load_json(path: str) -> dict:
    """json íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        st.warning(f"JSON ë¡œë“œ ì‹¤íŒ¨: {path} ({e})")
        return {}


@st.cache_data(show_spinner=False)
def load_stopwords_file(path: str) -> set[str]:
    """ë¶ˆìš©ì–´ íŒŒì¼ì„ ì½ì–´ setìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ set."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return {w.strip() for w in f if w.strip()}
    except FileNotFoundError:
        return set()
    except Exception:
        return set()


@st.cache_resource
def load_tokenizer():
    """ì‚¬ì „í•™ìŠµ í† í¬ë‚˜ì´ì €(pickle)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ None."""
    try:
        with open(TOKENIZER_PATH, "rb") as f:
            return pickle.load(f)
    except Exception as e:
        st.error(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


# ===================================================
# 2) í…ìŠ¤íŠ¸/ë¬¸ìì—´ ìœ í‹¸
# ===================================================
def clean_title(raw_title: str) -> str:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ titleì€ <b> íƒœê·¸ê°€ ì„ì—¬ì˜¤ëŠ” ê²½ìš°ê°€ ë§ì•„ ì œê±°í•©ë‹ˆë‹¤."""
    t = html.unescape(raw_title or "")
    t = re.sub(r"<.*?>", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def format_pubdate(pub_date: str) -> str:
    """RFC ë‚ ì§œ í˜•ì‹ì„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        dt = parsedate_to_datetime(pub_date)
        return dt.strftime("%Y-%m-%d %H:%M")
    except Exception:
        return pub_date or ""


@st.cache_data(show_spinner=False)
def clean_text_keep_korean(text: str) -> str:
    """
    í•œê¸€ ì¤‘ì‹¬ìœ¼ë¡œ ì •ì œí•©ë‹ˆë‹¤.
    - ìˆ«ì/ì˜ë¬¸/íŠ¹ìˆ˜ë¬¸ì ì œê±°
    - ê³µë°± ì •ë¦¬
    """
    text = re.sub(r"\d|[a-zA-Z]|\W", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def normalize_token(t: str) -> str:
    """í† í° ì •ê·œí™”(ê¸°í˜¸/ê³µë°± ì œê±°)."""
    if t is None:
        return ""
    t = str(t).strip()
    t = re.sub(r"[\"'â€œâ€â€˜â€™\(\)\[\]\{\},\.\!\?\:\;]", "", t)
    t = re.sub(r"\s+", "", t)
    return t


def build_final_keyword(category: str, user_keyword: str) -> str:
    """
    ë¶„ì•¼ + ì‚¬ìš©ì í‚¤ì›Œë“œë¥¼ ê²°í•©í•©ë‹ˆë‹¤.
    - ê³µë°±ì„ 1ê°œë¡œ ì •ë¦¬
    - ê²€ìƒ‰ ì•ˆì •ì„±ì„ ìœ„í•´ 'ë¶„ì•¼ + ê³µë°± + í‚¤ì›Œë“œ' í˜•íƒœë¥¼ ì‚¬ìš©
    """
    category = (category or "").strip()
    user_keyword = re.sub(r"\s+", " ", (user_keyword or "")).strip()
    return f"{category} {user_keyword}".strip()


def safe_filename(s: str) -> str:
    """
    íŒŒì¼ëª…ì— ë“¤ì–´ê°€ë©´ ìœ„í—˜í•œ ë¬¸ìë“¤ì„ '_'ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
    """
    s = s.strip()
    s = re.sub(r"[^\w\-ê°€-í£]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "result"


# ===================================================
# 3) ë„¤ì´ë²„ API í†µì‹ (ë°©ì–´ ì½”ë“œ í¬í•¨)
# ===================================================
def naver_news_api_request(keyword: str, display: int, start: int, client_id: str, client_secret: str):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í˜¸ì¶œ.
    - ì¸ì¦ ì‹¤íŒ¨/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜/HTTP ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•´ì„œ ì•±ì´ ì£½ì§€ ì•Šê²Œ í•¨
    - ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    if not client_id.strip() or not client_secret.strip():
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return []

    url = f"https://openapi.naver.com/v1/search/news.json?query={quote(keyword)}&display={display}&start={start}"
    headers = {
        "X-Naver-Client-Id": client_id.strip(),
        "X-Naver-Client-Secret": client_secret.strip(),
    }

    try:
        res = rq.get(url, headers=headers, timeout=10)
    except rq.exceptions.Timeout:
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤(timeout).")
        return []
    except rq.exceptions.ConnectionError:
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ConnectionError).")
        return []
    except rq.exceptions.RequestException as e:
        st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return []

    # âœ… ìš”êµ¬ì‚¬í•­ ë°˜ì˜: ìƒíƒœì½”ë“œê°€ 200ì´ ì•„ë‹ˆë©´ ì•ˆë‚´
    if res.status_code != 200:
        st.error("API ìš”ì²­ ì‹¤íŒ¨")  # ìš”êµ¬ì‚¬í•­ ë¬¸êµ¬
        # ì¸ì¦ ê´€ë ¨ì´ë©´ ë” ì¹œì ˆí•˜ê²Œ
        if res.status_code in (401, 403):
            st.warning("API ì¸ì¦ ì‹¤íŒ¨(ê¶Œí•œ/í‚¤ ì˜¤ë¥˜). Client ID/Secretì„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            st.warning(f"HTTP ìƒíƒœì½”ë“œ: {res.status_code}")
        return []

    try:
        data = res.json()
        return data.get("items", []) or []
    except Exception:
        st.error("API ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨")
        return []


def fetch_news_items(final_keyword: str, total_display: int, client_id: str, client_secret: str) -> list[dict]:
    """
    total_display(100~500)ë¥¼ 100ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì—¬ëŸ¬ ë²ˆ ìš”ì²­ í›„ itemsë¥¼ í•©ì¹©ë‹ˆë‹¤.
    - ì¼ë¶€ í˜ì´ì§€ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ í˜ì´ì§€ëŠ” ê³„ì† ì§„í–‰í•˜ë„ë¡ ì„¤ê³„
    """
    items: list[dict] = []
    page_count = max(1, total_display // 100)

    for i in range(page_count):
        start = 100 * i + 1
        page_items = naver_news_api_request(final_keyword, display=100, start=start,
                                            client_id=client_id, client_secret=client_secret)
        if page_items:
            items.extend(page_items)

    return items


def build_items_dataframe(items: list[dict]) -> pd.DataFrame:
    """
    itemsì—ì„œ title/pubDate/linkë§Œ ì¶”ì¶œí•˜ì—¬ DataFrame êµ¬ì„±.
    """
    rows = []
    for it in items:
        rows.append({
            "title": clean_title(it.get("title", "")),
            "pubDate": format_pubdate(it.get("pubDate", "")),
            "link": it.get("link", ""),
        })
    return pd.DataFrame(rows)


# ===================================================
# 4) í¬ë¡¤ë§(ì‹¤íŒ¨ ì‹œ skip)
# ===================================================
def crawl_naver_news_body(url: str) -> str:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸(#dic_area)ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    - ì‹¤íŒ¨í•˜ë©´ "" ë°˜í™˜(=skip)
    """
    try:
        res = rq.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        if res.status_code != 200:
            return ""
        soup = bs4.BeautifulSoup(res.text, "html.parser")
        tag = soup.select_one("#dic_area")
        return tag.get_text(separator=" ", strip=True) if tag else ""
    except rq.exceptions.RequestException:
        return ""
    except Exception:
        return ""


def collect_corpus_from_items(items: list[dict]) -> list[str]:
    """
    items ì¤‘ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ë§Œ ëŒ€ìƒìœ¼ë¡œ ë³¸ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    - í¬ë¡¤ë§ ì‹¤íŒ¨ëŠ” skip
    - ë„ˆë¬´ ì§§ì€ ë³¸ë¬¸ë„ skip
    """
    docs = []
    for it in items:
        link = it.get("link", "")
        if "n.news.naver" not in link:
            continue

        body = crawl_naver_news_body(link)
        if not body:
            continue

        cleaned = clean_text_keep_korean(body)
        if len(cleaned) < 100:
            continue

        docs.append(cleaned)

    return docs


# ===================================================
# 5) ë¶„ì„(soynlp ëª…ì‚¬ í•„í„° + TF-IDF)
# ===================================================
@st.cache_data(show_spinner=False)
def build_noun_set(docs_clean: list[str]) -> set[str]:
    """
    soynlpë¡œ ëª…ì‚¬ í›„ë³´ë¥¼ í•™ìŠµ/ì¶”ì¶œí•˜ì—¬ setìœ¼ë¡œ ë°˜í™˜.
    - ë°ì´í„°ê°€ ì ìœ¼ë©´ ë¹ˆ setì„ ë°˜í™˜(=ëª…ì‚¬ í•„í„° ì•½í™”)
    """
    sents = []
    for d in docs_clean:
        sents.extend([s.strip() for s in re.split(r"[\.!?]\s*|\n", d) if len(s.strip()) >= 10])

    if len(sents) < 5:
        return set()

    extractor = LRNounExtractor_v2(verbose=False)
    extractor.train(sents)
    nouns = extractor.extract()

    MIN_FREQ = 2
    MIN_SCORE = 0.4

    noun_set = set()
    for w, score in nouns.items():
        freq = getattr(score, "frequency", None)
        sc = getattr(score, "score", None)

        # ë²„ì „/êµ¬ì¡° ë°©ì–´
        if freq is None and isinstance(score, dict):
            freq = score.get("frequency")
        if sc is None and isinstance(score, dict):
            sc = score.get("score")

        if freq is not None and freq < MIN_FREQ:
            continue
        if sc is not None and sc < MIN_SCORE:
            continue
        noun_set.add(w)

    return noun_set


def tokenize_and_filter_docs(docs_clean: list[str], stopwords: set[str]) -> list[list[str]]:
    """
    - í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”
    - soynlp noun_set ê¸°ë°˜ìœ¼ë¡œ ëª…ì‚¬ë§Œ ë‚¨ê¹€
    - ë¶ˆìš©ì–´ ì œê±°
    """
    tokenizer = load_tokenizer()
    if tokenizer is None:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ë©´ ì•±ì´ ì£½ì§€ ì•Šë„ë¡ "ê³µë°± split"ìœ¼ë¡œ fallback
        st.warning("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¸í•´ ê°„ë‹¨í•œ split í† í°í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        noun_set = set()
        return [
            [t for t in d.split() if t not in stopwords and len(t) >= 2]
            for d in docs_clean
        ]

    noun_set = build_noun_set(docs_clean)
    if not noun_set:
        st.warning("ëª…ì‚¬ ì‚¬ì „ì´ ì•½í•©ë‹ˆë‹¤(ë§ë­‰ì¹˜ ë¶€ì¡±). ëª…ì‚¬ í•„í„°ê°€ ì™„í™”ë©ë‹ˆë‹¤.")

    docs_tokens = []
    for d in docs_clean:
        # flatten=False â†’ (left_token, right_token) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        try:
            toks = [t1 for t1, _ in tokenizer.tokenize(d, flatten=False)]
        except Exception:
            toks = d.split()

        filtered = []
        for t in toks:
            nt = normalize_token(t)
            if not nt:
                continue
            if not (2 <= len(nt) <= 8):
                continue
            if nt in stopwords:
                continue
            if noun_set and (nt not in noun_set):
                continue
            filtered.append(nt)

        docs_tokens.append(filtered)

    return docs_tokens


def compute_tfidf_scores(docs_tokens: list[list[str]], top_k: int = 80) -> dict[str, float]:
    """
    TF-IDFë¡œ í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    - ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë¹ˆ dict ë°˜í™˜
    """
    docs_str = [" ".join(ts) for ts in docs_tokens if ts]
    if len(docs_str) < 2:
        return {}

    try:
        vec = TfidfVectorizer(
            tokenizer=str.split,
            token_pattern=None,
            lowercase=False,
            min_df=2,
        )
        X = vec.fit_transform(docs_str)

        terms = np.array(vec.get_feature_names_out())
        scores = np.asarray(X.sum(axis=0)).ravel()
        if scores.size == 0:
            return {}

        idx = np.argsort(scores)[::-1][:top_k]
        return {terms[i]: float(scores[i]) for i in idx}
    except ValueError:
        # min_df=2 ì¡°ê±´ ë“±ìœ¼ë¡œ ë‹¨ì–´ê°€ í•˜ë‚˜ë„ ì•ˆ ë‚¨ëŠ” ê²½ìš°
        return {}
    except Exception as e:
        st.error(f"TF-IDF ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def build_keyword_tables(score_dict: dict[str, float]) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    score_dictë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ê³  Top50/Top20ì„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    df_kw = (
        pd.DataFrame(list(score_dict.items()), columns=["keyword", "score"])
        .sort_values("score", ascending=False)
    )
    return df_kw, df_kw.head(50).copy(), df_kw.head(20).copy()


# ===================================================
# 6) ì‹œê°í™”(figure ë°˜í™˜)
# ===================================================
def make_wordcloud_figure(freq: dict[str, float], mask_name: str):
    """
    ì›Œë“œí´ë¼ìš°ë“œ figureë¥¼ ìƒì„±í•´ ë°˜í™˜í•©ë‹ˆë‹¤.
    - freqê°€ ë¹„ì–´ìˆìœ¼ë©´ None
    """
    if not freq:
        return None

    bg_path = MASK_BG.get(mask_name, MASK_BG["ì—†ìŒ"])
    try:
        img = Image.open(bg_path)
        mask = np.array(img)
    except Exception:
        mask = None

    wc = WordCloud(
        font_path=FONT_PATH,
        background_color="white",
        max_words=80,
        mask=mask,
    ).generate_from_frequencies(freq)

    fig = plt.figure(figsize=(10, 10))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    return fig


def make_top20_bar_figure(df_top20: pd.DataFrame):
    """
    Top20 ë§‰ëŒ€ì°¨íŠ¸ figureë¥¼ ìƒì„±í•´ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if df_top20.empty:
        return None

    fig = plt.figure(figsize=(10, 5))
    plt.bar(df_top20["keyword"], df_top20["score"])
    plt.xticks(rotation=45, ha="right")
    plt.title("TF-IDF ìƒìœ„ í‚¤ì›Œë“œ (Top 20)")
    plt.tight_layout()
    return fig


# ===================================================
# 7) ë‹¤ìš´ë¡œë“œ(ì´ë¯¸ì§€ 2ê°œë¥¼ ZIPìœ¼ë¡œ ì œê³µ)
# ===================================================
def fig_to_png_bytes(fig) -> bytes:
    buf = BytesIO()
    fig.savefig(buf, format="png", dpi=160, bbox_inches="tight")
    buf.seek(0)
    return buf.getvalue()


def make_images_zip_bytes(wc_fig, top20_fig, base_name: str) -> bytes:
    """
    ë²„íŠ¼ 1ê°œë¡œ ì›Œë“œí´ë¼ìš°ë“œ + Top20 ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë‚´ë ¤ë°›ê¸° ìœ„í•´ ZIPìœ¼ë¡œ ë¬¶ìŠµë‹ˆë‹¤.
    """
    zip_buf = BytesIO()
    with zipfile.ZipFile(zip_buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr(f"{base_name}_wordcloud.png", fig_to_png_bytes(wc_fig))
        zf.writestr(f"{base_name}_top20.png", fig_to_png_bytes(top20_fig))
    zip_buf.seek(0)
    return zip_buf.getvalue()


# ===================================================
# 8) UI ë Œë”ë§ í•¨ìˆ˜ë“¤
# ===================================================
def render_header_with_lottie():
    """íƒ€ì´í‹€ ì˜† Lottie + íƒ€ì´í‹€ ì¶œë ¥"""
    col1, col2 = st.columns([1, 2])
    with col1:
        lottie = load_json(LOTTIE_PATH)
        if lottie:
            st_lottie(lottie, speed=1, loop=True, width=200, height=200)
    with col2:
        st.write("")
        st.write("")
        st.write("")
        st.title("ë‰´ìŠ¤ í‚¤ì›Œë“œ ì‹œê°í™”")


def render_sidebar_api_settings():
    """ì‚¬ì´ë“œë°” API ì„¤ì • ì…ë ¥ UI"""
    st.session_state.setdefault("client_id", "")
    st.session_state.setdefault("client_secret", "")

    with st.sidebar.form("client_settings", clear_on_submit=False):
        st.header("API ì„¤ì •")
        cid = st.text_input("Client ID:", value=st.session_state["client_id"])
        secret = st.text_input("Client Secret:", type="password", value=st.session_state["client_secret"])
        if st.form_submit_button("OK"):
            st.session_state["client_id"] = (cid or "").strip()
            st.session_state["client_secret"] = (secret or "").strip()
            st.rerun()


def render_sidebar_stopwords() -> set[str]:
    """
    ì‚¬ì´ë“œë°” ë¶ˆìš©ì–´ ì…ë ¥ UI
    - íŒŒì¼ ë¶ˆìš©ì–´ + ì‚¬ìš©ì ì¶”ê°€ ë¶ˆìš©ì–´ í•©ì³ì„œ ë°˜í™˜
    """
    st.sidebar.header("ë¶ˆìš©ì–´(Stopwords)")
    base_stop = load_stopwords_file(STOPWORDS_PATH)
    extra_stop = st.sidebar.text_area("ì¶”ê°€ ë¶ˆìš©ì–´(ì¤„ë°”ê¿ˆìœ¼ë¡œ ì…ë ¥)", value="", height=120)
    extra_stop_set = {w.strip() for w in extra_stop.splitlines() if w.strip()}
    stopwords = base_stop | extra_stop_set
    st.sidebar.caption(f"í˜„ì¬ ë¶ˆìš©ì–´ ìˆ˜: {len(stopwords)} (íŒŒì¼ + ì¶”ê°€ ì…ë ¥)")
    return stopwords


def render_main_form():
    """
    ë©”ì¸ ì…ë ¥ í¼ UI
    - ì²´í¬ë°•ìŠ¤ ë°°ì¹˜ ìš”êµ¬ì‚¬í•­ ë°˜ì˜
    """
    with st.form("search", clear_on_submit=False):
        category = st.selectbox("ë¶„ì•¼:", ["ê²½ì œ", "ì •ì¹˜", "ì‚¬íšŒ", "êµ­ì œ", "ì—°ì˜ˆ", "IT", "ë¬¸í™”"])
        user_keyword = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ(í•„ìˆ˜):", value="", placeholder="ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI, ë©”íƒ€ë²„ìŠ¤ ...")
        display = st.select_slider("ë¶„ëŸ‰(ê¸°ì‚¬ ìˆ˜):", options=[100, 200, 300, 400, 500], value=100)
        mask = st.radio("ë°±ë§ˆìŠ¤í¬:", ["ì—†ìŒ", "íƒ€ì›", "ë§í’ì„ ", "í•˜íŠ¸"], horizontal=True)

        # 1ì¤„: ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°, ë§í¬ ì œê³µ, ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)
        r1c1, r1c2, r1c3 = st.columns([1, 1, 1])
        with r1c1:
            show_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°", value=True)
        with r1c2:
            show_links = st.checkbox("ë§í¬ ì œê³µ", value=False)
        with r1c3:
            dl_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)", value=False)

        # 2ì¤„: í‚¤ì›Œë“œ í‘œ ë³´ê¸°, í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv), ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)
        r2c1, r2c2, r2c3 = st.columns([1, 1, 1])
        with r2c1:
            show_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë³´ê¸°", value=True)
        with r2c2:
            dl_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)", value=False)
        with r2c3:
            dl_images = st.checkbox("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)", value=False)

        submitted = st.form_submit_button("OK")

    return {
        "category": category,
        "user_keyword": user_keyword,
        "display": display,
        "mask": mask,
        "show_articles": show_articles,
        "show_links": show_links,
        "dl_articles": dl_articles,
        "show_keywords": show_keywords,
        "dl_keywords": dl_keywords,
        "dl_images": dl_images,
        "submitted": submitted,
    }


# ===================================================
# 9) ë©”ì¸ ì‹¤í–‰ ë¡œì§(ì•±ì˜ íë¦„)
# ===================================================
def run_app():
    # 1) UI ê¸°ë³¸
    st.set_page_config(page_title="ë‰´ìŠ¤ í‚¤ì›Œë“œ ì‹œê°í™”", layout="wide")
    setup_matplotlib_korean_font()
    render_header_with_lottie()

    # 2) Sidebar
    render_sidebar_api_settings()
    stopwords = render_sidebar_stopwords()

    # 3) Form
    form = render_main_form()
    if not form["submitted"]:
        return

    # 4) ì…ë ¥ ê²€ì¦(ë°ì´í„° ë¶€ì¡± ì•ˆë‚´ ê°•í™”)
    if not form["user_keyword"].strip():
        st.warning("ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI)")
        return

    client_id = st.session_state.get("client_id", "").strip()
    client_secret = st.session_state.get("client_secret", "").strip()
    if not client_id or not client_secret:
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ Client ID/Secretì„ ì…ë ¥ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return

    final_keyword = build_final_keyword(form["category"], form["user_keyword"])

    # 5) ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ (API ì˜¤ë¥˜/ì¸ì¦ ì‹¤íŒ¨ ë°©ì–´)
    st.info(f"ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ ì¤‘... (ê²€ìƒ‰ì–´: {final_keyword})")
    items = fetch_news_items(final_keyword, form["display"], client_id, client_secret)

    if not items:
        st.warning("ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.info("ê°€ëŠ¥í•œ ì›ì¸: (1) ì¸ì¦ ì‹¤íŒ¨ (2) ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (3) ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return

    df_items = build_items_dataframe(items)

    # 6) ê¸°ì‚¬ ëª©ë¡ í‘œì‹œ/ë§í¬
    if form["show_articles"]:
        st.subheader("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡")
        if df_items.empty:
            st.warning("ê¸°ì‚¬ ëª©ë¡ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.dataframe(df_items[["title", "pubDate"]], use_container_width=True)

            if form["show_links"]:
                st.caption("ê¸°ì‚¬ ë§í¬(í´ë¦­):")
                # ë„ˆë¬´ ë§ìœ¼ë©´ ë¶€ë‹´ì´ ë  ìˆ˜ ìˆì–´ ìƒìœ„ 30ê°œë§Œ
                for _, r in df_items.head(30).iterrows():
                    if r["link"]:
                        st.markdown(f"- [ğŸ”— ë°”ë¡œê°€ê¸°]({r['link']}) â€” {r['title']}")

    # 7) ë³¸ë¬¸ í¬ë¡¤ë§ (ì‹¤íŒ¨ ì‹œ skip)
    st.info("ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
    docs_clean = collect_corpus_from_items(items)

    if len(docs_clean) < 5:
        st.warning("ë³¸ë¬¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.")
        st.info(
            "ê°œì„  íŒ:\n"
            "- ë¶„ëŸ‰ì„ 200~500ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”.\n"
            "- í‚¤ì›Œë“œë¥¼ ë” ë„“ê²Œ/ì¼ë°˜ì ìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”.\n"
            "- ë§í¬ ì œê³µì„ ì¼œì„œ ì‹¤ì œë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ê°€ ë§ì€ì§€ í™•ì¸í•´ë³´ì„¸ìš”."
        )
        return

    # 8) í† í°í™”/í•„í„° + TF-IDF
    st.info("í‚¤ì›Œë“œ ë¶„ì„ ì¤‘(ëª…ì‚¬ í•„í„° + TF-IDF)...")
    docs_tokens = tokenize_and_filter_docs(docs_clean, stopwords)

    score_dict = compute_tfidf_scores(docs_tokens, top_k=80)
    if not score_dict:
        st.warning("í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ë°ì´í„°/í•„í„° ì¡°ê±´ ë¶€ì¡±).")
        st.info(
            "ê°œì„  íŒ:\n"
            "- ë¶ˆìš©ì–´ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ë‹¨ì–´ê°€ ê±°ì˜ ë‚¨ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
            "- ë¶„ëŸ‰ì„ ëŠ˜ë¦¬ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ ë°”ê¿” ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
        )
        return

    df_kw, df_kw_top50, df_kw_top20 = build_keyword_tables(score_dict)

    # 9) í‚¤ì›Œë“œ í‘œ í‘œì‹œ
    if form["show_keywords"]:
        st.subheader("í‚¤ì›Œë“œ(TF-IDF) ìƒìœ„ 50")
        st.dataframe(df_kw_top50, use_container_width=True)

    # 10) ì‹œê°í™”
    st.info("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘...")
    wc_fig = make_wordcloud_figure(score_dict, form["mask"])
    if wc_fig is None:
        st.warning("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨(ë°ì´í„° ë¶€ì¡±).")
        return
    st.pyplot(wc_fig)

    st.info("Top20 ë§‰ëŒ€ì°¨íŠ¸ ìƒì„± ì¤‘...")
    top20_fig = make_top20_bar_figure(df_kw_top20)
    if top20_fig is None:
        st.warning("Top20 ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨(ë°ì´í„° ë¶€ì¡±).")
        return
    st.pyplot(top20_fig)

    # 11) ë‹¤ìš´ë¡œë“œ(ì°¨íŠ¸ ì•„ë˜, ë²„íŠ¼ í•œ ì¤„ ë°°ì¹˜)
    st.markdown("---")
    st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")

    can_articles = not df_items.empty
    can_keywords = not df_kw_top50.empty
    can_images = (wc_fig is not None) and (top20_fig is not None)

    base = safe_filename(final_keyword)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    c1, c2, c3 = st.columns(3)

    with c1:
        st.download_button(
            label="ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)",
            data=df_items.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig") if can_articles else b"",
            file_name=f"articles_{base}_{ts}.csv",
            mime="text/csv",
            disabled=not (form["dl_articles"] and can_articles),
        )

    with c2:
        st.download_button(
            label="í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)",
            data=df_kw_top50.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig") if can_keywords else b"",
            file_name=f"keywords_{base}_{ts}.csv",
            mime="text/csv",
            disabled=not (form["dl_keywords"] and can_keywords),
        )

    with c3:
        # ë²„íŠ¼ ë¬¸êµ¬ëŠ” .pngë¡œ ë³´ì´ì§€ë§Œ 2ì¥ ë™ì‹œ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ ZIP ì œê³µ(ì•ˆì •ì )
        zip_bytes = make_images_zip_bytes(wc_fig, top20_fig, f"{base}_{ts}") if can_images else b""
        st.download_button(
            label="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)",
            data=zip_bytes,
            file_name=f"images_{base}_{ts}.zip",
            mime="application/zip",
            disabled=not (form["dl_images"] and can_images),
        )


# ===================================================
# ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# ===================================================
if __name__ == "__main__":
    run_app()
