# app_v3_timeout_fixed.py
#
# ===================================================
# News Keyword Visualizer V3 (Timeout Hardened)
# ---------------------------------------------------
# âœ… íƒ€ì„ì•„ì›ƒ ë¬¸ì œ ì™„í™” ì¶”ê°€
#  - requests.Session + ì—°ê²° ì¬ì‚¬ìš©(ì†ë„/ì•ˆì •ì„±â†‘)
#  - Retry + Exponential Backoff(+Jitter)
#  - API/Crawl íƒ€ì„ì•„ì›ƒ ë¶„ë¦¬
#  - ë³¸ë¬¸ í¬ë¡¤ë§ ì œí•œ ë³‘ë ¬(ThreadPool)ë¡œ ì´ ì†Œìš”ì‹œê°„ ë‹¨ì¶•
# ===================================================

import json
import re
import pickle
import html
import time
import random
from datetime import datetime
from email.utils import parsedate_to_datetime
from io import BytesIO
from urllib.parse import quote
import zipfile
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests as rq
from requests.adapters import HTTPAdapter

import bs4
import pandas as pd
import numpy as np

import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from PIL import Image
from wordcloud import WordCloud
from streamlit_lottie import st_lottie

from sklearn.feature_extraction.text import TfidfVectorizer
from soynlp.noun import LRNounExtractor_v2


# ===================================================
# 0) ì „ì—­ ì„¤ì •(í°íŠ¸, ê²½ë¡œ)
# ===================================================
FONT_PATH = "./resources/NanumSquareR.ttf"
STOPWORDS_PATH = "./resources/stopwords_ko.txt"
TOKENIZER_PATH = "./resources/my_tokenizer1.model"
LOTTIE_PATH = "./resources/lottie-full-movie-experience-including-music-news-video-weather-and-lots-of-entertainment.json"

MASK_BG = {
    "ì—†ìŒ": "./resources/background_0.png",
    "íƒ€ì›": "./resources/background_1.png",
    "ë§í’ì„ ": "./resources/background_2.png",
    "í•˜íŠ¸": "./resources/background_3.png",
}

# ===================================================
# âœ… íƒ€ì„ì•„ì›ƒ/ì¬ì‹œë„/ë³‘ë ¬ ì„¤ì • (í•µì‹¬!)
# ===================================================
TIMEOUT_API = 6          # APIëŠ” ì§§ê²Œ (ë°°í¬í™˜ê²½ì—ì„œ ë„ˆë¬´ ê¸¸ê²Œ ì¡ìœ¼ë©´ ì „ì²´ ì§€ì—°)
TIMEOUT_CRAWL = 8        # ë³¸ë¬¸ í¬ë¡¤ë§ì€ ì¡°ê¸ˆ ë” ì—¬ìœ 
MAX_RETRIES = 3          # ì¬ì‹œë„ íšŸìˆ˜
BACKOFF_BASE = 0.8       # ë°±ì˜¤í”„ ê¸°ë³¸(ì´ˆ) - 0.8, 1.6, 3.2 ... + jitter
MAX_WORKERS = 6          # í¬ë¡¤ë§ ë³‘ë ¬ ìˆ˜(ë„ˆë¬´ ë†’ì´ë©´ ì°¨ë‹¨/ë¶ˆì•ˆì •)

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"


def setup_matplotlib_korean_font() -> None:
    """
    ë§‰ëŒ€ì°¨íŠ¸ ë“± matplotlib ì¶œë ¥ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê²Œ í°íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    í°íŠ¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìœˆë„ìš° ê¸°ë³¸ í°íŠ¸(Malgun Gothic)ë¡œ fallback í•©ë‹ˆë‹¤.
    """
    try:
        fm.fontManager.addfont(FONT_PATH)
        plt.rcParams["font.family"] = fm.FontProperties(fname=FONT_PATH).get_name()
    except Exception:
        plt.rcParams["font.family"] = "Malgun Gothic"
    plt.rcParams["axes.unicode_minus"] = False


# ===================================================
# âœ… (ì¶”ê°€) requests ì„¸ì…˜ ìƒì„±: ì—°ê²° ì¬ì‚¬ìš© + ê¸°ë³¸ retry(ì—°ê²° ë ˆë²¨)
# ===================================================
@st.cache_resource
def create_http_session() -> rq.Session:
    """
    Streamlit Cloudì—ì„œëŠ” ë„¤íŠ¸ì›Œí¬ ë³€ë™/ì—°ê²° ë¶ˆì•ˆì •ì´ ì¢…ì¢… ìˆìŠµë‹ˆë‹¤.
    Session + HTTPAdapterë¡œ ì—°ê²° ì¬ì‚¬ìš© + ê¸°ë³¸ ì¬ì‹œë„(ì—°ê²°ë‹¨)ë¥¼ ì„¤ì •í•´ ì•ˆì •ì„±ì„ ì˜¬ë¦½ë‹ˆë‹¤.

    â€» ì—¬ê¸°ì„œëŠ” urllib3 Retryë¥¼ ì§ì ‘ ì“°ì§€ ì•Šê³ ,
      ì•„ë˜ request_with_retry()ì—ì„œ ì•± ë ˆë²¨ ì¬ì‹œë„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
      (ë°°í¬í™˜ê²½ì—ì„œ ì œì–´/ë¡œê·¸ê°€ ë” ì‰¬ì›€)
    """
    s = rq.Session()
    adapter = HTTPAdapter(
        pool_connections=20,
        pool_maxsize=20,
        max_retries=0,  # ì•± ë ˆë²¨ì—ì„œ ì¬ì‹œë„í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” 0
    )
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s


# ===================================================
# âœ… (ì¶”ê°€) ì¬ì‹œë„ + ë°±ì˜¤í”„(+jitter) ìš”ì²­ ë˜í¼
# ===================================================
def request_with_retry(
    session: rq.Session,
    method: str,
    url: str,
    *,
    headers: dict | None = None,
    timeout: int = 10,
    max_retries: int = MAX_RETRIES,
) -> rq.Response | None:
    """
    íƒ€ì„ì•„ì›ƒ/ì¼ì‹œ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë¥¼ ë§Œë‚˜ë„ ê³§ë°”ë¡œ ì‹¤íŒ¨í•˜ì§€ ì•Šê³  ì¬ì‹œë„í•©ë‹ˆë‹¤.

    - Exponential Backoff: BACKOFF_BASE * (2 ** attempt)
    - Jitter: ëœë¤(0~0.3ì´ˆ) ì¶”ê°€ â†’ ë°°í¬ í™˜ê²½ì—ì„œ ë™ì‹œ ì¬ì‹œë„ ì¶©ëŒ ì™„í™”
    """
    last_exc = None

    for attempt in range(max_retries + 1):
        try:
            res = session.request(method, url, headers=headers, timeout=timeout)
            return res
        except (rq.exceptions.Timeout, rq.exceptions.ConnectionError, rq.exceptions.RequestException) as e:
            last_exc = e

            # ë§ˆì§€ë§‰ ì‹œë„ë©´ ì¢…ë£Œ
            if attempt == max_retries:
                break

            sleep_sec = (BACKOFF_BASE * (2 ** attempt)) + random.uniform(0, 0.3)
            time.sleep(sleep_sec)

    # ì‹¤íŒ¨ ì‹œ None ë°˜í™˜(ìƒìœ„ì—ì„œ ì‚¬ìš©ì ì•ˆë‚´/skip ì²˜ë¦¬)
    return None


# ===================================================
# 1) ë¡œë”©/ìºì‹œ í•¨ìˆ˜ë“¤(ë¦¬ì†ŒìŠ¤)
# ===================================================
def load_json(path: str) -> dict:
    """json íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        st.warning(f"JSON ë¡œë“œ ì‹¤íŒ¨: {path} ({e})")
        return {}


@st.cache_data(show_spinner=False)
def load_stopwords_file(path: str) -> set[str]:
    """ë¶ˆìš©ì–´ íŒŒì¼ì„ ì½ì–´ setìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ set."""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return {w.strip() for w in f if w.strip()}
    except FileNotFoundError:
        return set()
    except Exception:
        return set()


@st.cache_resource
def load_tokenizer():
    """ì‚¬ì „í•™ìŠµ í† í¬ë‚˜ì´ì €(pickle)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ None."""
    try:
        with open(TOKENIZER_PATH, "rb") as f:
            return pickle.load(f)
    except Exception as e:
        st.error(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


# ===================================================
# 2) í…ìŠ¤íŠ¸/ë¬¸ìì—´ ìœ í‹¸
# ===================================================
def clean_title(raw_title: str) -> str:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ titleì€ <b> íƒœê·¸ê°€ ì„ì—¬ì˜¤ëŠ” ê²½ìš°ê°€ ë§ì•„ ì œê±°í•©ë‹ˆë‹¤."""
    t = html.unescape(raw_title or "")
    t = re.sub(r"<.*?>", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def format_pubdate(pub_date: str) -> str:
    """RFC ë‚ ì§œ í˜•ì‹ì„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        dt = parsedate_to_datetime(pub_date)
        return dt.strftime("%Y-%m-%d %H:%M")
    except Exception:
        return pub_date or ""


@st.cache_data(show_spinner=False)
def clean_text_keep_korean(text: str) -> str:
    """
    í•œê¸€ ì¤‘ì‹¬ìœ¼ë¡œ ì •ì œí•©ë‹ˆë‹¤.
    - ìˆ«ì/ì˜ë¬¸/íŠ¹ìˆ˜ë¬¸ì ì œê±°
    - ê³µë°± ì •ë¦¬
    """
    text = re.sub(r"\d|[a-zA-Z]|\W", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def normalize_token(t: str) -> str:
    """í† í° ì •ê·œí™”(ê¸°í˜¸/ê³µë°± ì œê±°)."""
    if t is None:
        return ""
    t = str(t).strip()
    t = re.sub(r"[\"'â€œâ€â€˜â€™\(\)\[\]\{\},\.\!\?\:\;]", "", t)
    t = re.sub(r"\s+", "", t)
    return t


def build_final_keyword(category: str, user_keyword: str) -> str:
    """ë¶„ì•¼ + ì‚¬ìš©ì í‚¤ì›Œë“œ ê²°í•©"""
    category = (category or "").strip()
    user_keyword = re.sub(r"\s+", " ", (user_keyword or "")).strip()
    return f"{category} {user_keyword}".strip()


def safe_filename(s: str) -> str:
    """íŒŒì¼ëª… ì•ˆì „í™”"""
    s = s.strip()
    s = re.sub(r"[^\w\-ê°€-í£]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "result"


# ===================================================
# 3) ë„¤ì´ë²„ API í†µì‹ (íƒ€ì„ì•„ì›ƒ ê°•í™”)
# ===================================================
def naver_news_api_request(keyword: str, display: int, start: int, client_id: str, client_secret: str):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í˜¸ì¶œ.
    - request_with_retry ì ìš©
    - ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    """
    if not client_id.strip() or not client_secret.strip():
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return []

    url = f"https://openapi.naver.com/v1/search/news.json?query={quote(keyword)}&display={display}&start={start}"
    headers = {
        "X-Naver-Client-Id": client_id.strip(),
        "X-Naver-Client-Secret": client_secret.strip(),
        "User-Agent": USER_AGENT,
    }

    session = create_http_session()
    res = request_with_retry(session, "GET", url, headers=headers, timeout=TIMEOUT_API)

    if res is None:
        st.error("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆê±°ë‚˜ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤(timeout/connection).")
        return []

    if res.status_code != 200:
        st.error("API ìš”ì²­ ì‹¤íŒ¨")
        if res.status_code in (401, 403):
            st.warning("API ì¸ì¦ ì‹¤íŒ¨(ê¶Œí•œ/í‚¤ ì˜¤ë¥˜). Client ID/Secretì„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            st.warning(f"HTTP ìƒíƒœì½”ë“œ: {res.status_code}")
        return []

    try:
        data = res.json()
        return data.get("items", []) or []
    except Exception:
        st.error("API ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨")
        return []


def fetch_news_items(final_keyword: str, total_display: int, client_id: str, client_secret: str) -> list[dict]:
    """100ë‹¨ìœ„ë¡œ ìš”ì²­ í›„ í•©ì¹˜ê¸°(ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš©)"""
    items: list[dict] = []
    page_count = max(1, total_display // 100)

    for i in range(page_count):
        start = 100 * i + 1
        page_items = naver_news_api_request(final_keyword, 100, start, client_id, client_secret)
        if page_items:
            items.extend(page_items)

        # âœ… í˜ì´ì§€ë§ˆë‹¤ ì•„ì£¼ ì§§ê²Œ ì‰¬ì–´ì£¼ë©´(íŠ¹íˆ ë°°í¬ í™˜ê²½) ì‹¤íŒ¨ìœ¨ì´ ì¤„ì–´ë“­ë‹ˆë‹¤.
        time.sleep(0.15)

    return items


def build_items_dataframe(items: list[dict]) -> pd.DataFrame:
    """itemsì—ì„œ title/pubDate/linkë§Œ ì¶”ì¶œ"""
    rows = []
    for it in items:
        rows.append({
            "title": clean_title(it.get("title", "")),
            "pubDate": format_pubdate(it.get("pubDate", "")),
            "link": it.get("link", ""),
        })
    return pd.DataFrame(rows)


# ===================================================
# 4) í¬ë¡¤ë§(íƒ€ì„ì•„ì›ƒ ê°•í™” + ì œí•œ ë³‘ë ¬)
# ===================================================
def crawl_naver_news_body(url: str) -> str:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸(#dic_area) í¬ë¡¤ë§
    - request_with_retry ì ìš©
    - ì‹¤íŒ¨ ì‹œ "" ë°˜í™˜(=skip)
    """
    session = create_http_session()
    headers = {"User-Agent": USER_AGENT}

    res = request_with_retry(session, "GET", url, headers=headers, timeout=TIMEOUT_CRAWL)

    if res is None:
        return ""

    if res.status_code != 200:
        return ""

    try:
        soup = bs4.BeautifulSoup(res.text, "html.parser")
        tag = soup.select_one("#dic_area")
        return tag.get_text(separator=" ", strip=True) if tag else ""
    except Exception:
        return ""


def collect_corpus_from_items(items: list[dict]) -> list[str]:
    """
    âœ… ê°œì„  í¬ì¸íŠ¸
    - í¬ë¡¤ë§ì„ 'ì œí•œëœ ë³‘ë ¬'ë¡œ ìˆ˜í–‰ â†’ ì „ì²´ ì†Œìš”ì‹œê°„ ê°ì†Œ â†’ ì²´ê° timeout ê°ì†Œ
    - ì‹¤íŒ¨/ì§§ì€ ë³¸ë¬¸ì€ skip
    """
    links = []
    for it in items:
        link = it.get("link", "")
        if "n.news.naver" in link:
            links.append(link)

    if not links:
        return []

    docs: list[str] = []

    # ThreadPoolë¡œ ë³‘ë ¬ í¬ë¡¤ë§(ë™ì‹œ ìš”ì²­ ìˆ˜ëŠ” MAX_WORKERSë¡œ ì œí•œ)
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = {ex.submit(crawl_naver_news_body, url): url for url in links}

        for fut in as_completed(futures):
            body = ""
            try:
                body = fut.result()
            except Exception:
                body = ""

            if not body:
                continue

            cleaned = clean_text_keep_korean(body)
            if len(cleaned) < 100:
                continue

            docs.append(cleaned)

    return docs


# ===================================================
# 5) ë¶„ì„(soynlp ëª…ì‚¬ í•„í„° + TF-IDF)
# ===================================================
@st.cache_data(show_spinner=False)
def build_noun_set(docs_clean: list[str]) -> set[str]:
    """soynlp ëª…ì‚¬ í›„ë³´ set"""
    sents = []
    for d in docs_clean:
        sents.extend([s.strip() for s in re.split(r"[\.!?]\s*|\n", d) if len(s.strip()) >= 10])

    if len(sents) < 5:
        return set()

    extractor = LRNounExtractor_v2(verbose=False)
    extractor.train(sents)
    nouns = extractor.extract()

    MIN_FREQ = 2
    MIN_SCORE = 0.4

    noun_set = set()
    for w, score in nouns.items():
        freq = getattr(score, "frequency", None)
        sc = getattr(score, "score", None)

        if freq is None and isinstance(score, dict):
            freq = score.get("frequency")
        if sc is None and isinstance(score, dict):
            sc = score.get("score")

        if freq is not None and freq < MIN_FREQ:
            continue
        if sc is not None and sc < MIN_SCORE:
            continue
        noun_set.add(w)

    return noun_set


def tokenize_and_filter_docs(docs_clean: list[str], stopwords: set[str]) -> list[list[str]]:
    """í† í°í™” + ëª…ì‚¬ í•„í„° + ë¶ˆìš©ì–´ ì œê±°"""
    tokenizer = load_tokenizer()
    if tokenizer is None:
        st.warning("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¸í•´ ê°„ë‹¨í•œ split í† í°í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        noun_set = set()
        return [
            [t for t in d.split() if t not in stopwords and len(t) >= 2]
            for d in docs_clean
        ]

    noun_set = build_noun_set(docs_clean)
    if not noun_set:
        st.warning("ëª…ì‚¬ ì‚¬ì „ì´ ì•½í•©ë‹ˆë‹¤(ë§ë­‰ì¹˜ ë¶€ì¡±). ëª…ì‚¬ í•„í„°ê°€ ì™„í™”ë©ë‹ˆë‹¤.")

    docs_tokens = []
    for d in docs_clean:
        try:
            toks = [t1 for t1, _ in tokenizer.tokenize(d, flatten=False)]
        except Exception:
            toks = d.split()

        filtered = []
        for t in toks:
            nt = normalize_token(t)
            if not nt:
                continue
            if not (2 <= len(nt) <= 8):
                continue
            if nt in stopwords:
                continue
            if noun_set and (nt not in noun_set):
                continue
            filtered.append(nt)

        docs_tokens.append(filtered)

    return docs_tokens


def compute_tfidf_scores(docs_tokens: list[list[str]], top_k: int = 80) -> dict[str, float]:
    """TF-IDF ì ìˆ˜ ê³„ì‚°"""
    docs_str = [" ".join(ts) for ts in docs_tokens if ts]
    if len(docs_str) < 2:
        return {}

    try:
        vec = TfidfVectorizer(
            tokenizer=str.split,
            token_pattern=None,
            lowercase=False,
            min_df=2,
        )
        X = vec.fit_transform(docs_str)

        terms = np.array(vec.get_feature_names_out())
        scores = np.asarray(X.sum(axis=0)).ravel()
        if scores.size == 0:
            return {}

        idx = np.argsort(scores)[::-1][:top_k]
        return {terms[i]: float(scores[i]) for i in idx}
    except ValueError:
        return {}
    except Exception as e:
        st.error(f"TF-IDF ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def build_keyword_tables(score_dict: dict[str, float]):
    df_kw = (
        pd.DataFrame(list(score_dict.items()), columns=["keyword", "score"])
        .sort_values("score", ascending=False)
    )
    return df_kw, df_kw.head(50).copy(), df_kw.head(20).copy()


# ===================================================
# 6) ì‹œê°í™”(figure ë°˜í™˜)
# ===================================================
def make_wordcloud_figure(freq: dict[str, float], mask_name: str):
    if not freq:
        return None

    bg_path = MASK_BG.get(mask_name, MASK_BG["ì—†ìŒ"])
    try:
        img = Image.open(bg_path)
        mask = np.array(img)
    except Exception:
        mask = None

    wc = WordCloud(
        font_path=FONT_PATH,
        background_color="white",
        max_words=80,
        mask=mask,
    ).generate_from_frequencies(freq)

    fig = plt.figure(figsize=(10, 10))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    return fig


def make_top20_bar_figure(df_top20: pd.DataFrame):
    if df_top20.empty:
        return None

    fig = plt.figure(figsize=(10, 5))
    plt.bar(df_top20["keyword"], df_top20["score"])
    plt.xticks(rotation=45, ha="right")
    plt.title("TF-IDF ìƒìœ„ í‚¤ì›Œë“œ (Top 20)")
    plt.tight_layout()
    return fig


# ===================================================
# 7) ë‹¤ìš´ë¡œë“œ(ì´ë¯¸ì§€ 2ê°œë¥¼ ZIPìœ¼ë¡œ ì œê³µ)
# ===================================================
def fig_to_png_bytes(fig) -> bytes:
    buf = BytesIO()
    fig.savefig(buf, format="png", dpi=160, bbox_inches="tight")
    buf.seek(0)
    return buf.getvalue()


def make_images_zip_bytes(wc_fig, top20_fig, base_name: str) -> bytes:
    zip_buf = BytesIO()
    with zipfile.ZipFile(zip_buf, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr(f"{base_name}_wordcloud.png", fig_to_png_bytes(wc_fig))
        zf.writestr(f"{base_name}_top20.png", fig_to_png_bytes(top20_fig))
    zip_buf.seek(0)
    return zip_buf.getvalue()


# ===================================================
# 8) UI ë Œë”ë§ í•¨ìˆ˜ë“¤
# ===================================================
def render_header_with_lottie():
    col1, col2 = st.columns([1, 2])
    with col1:
        lottie = load_json(LOTTIE_PATH)
        if lottie:
            st_lottie(lottie, speed=1, loop=True, width=200, height=200)
    with col2:
        st.write("")
        st.write("")
        st.write("")
        st.title("ë‰´ìŠ¤ í‚¤ì›Œë“œ ì‹œê°í™”")


def render_sidebar_api_settings():
    st.session_state.setdefault("client_id", "")
    st.session_state.setdefault("client_secret", "")

    with st.sidebar.form("client_settings", clear_on_submit=False):
        st.header("API ì„¤ì •")
        cid = st.text_input("Client ID:", value=st.session_state["client_id"])
        secret = st.text_input("Client Secret:", type="password", value=st.session_state["client_secret"])
        if st.form_submit_button("OK"):
            st.session_state["client_id"] = (cid or "").strip()
            st.session_state["client_secret"] = (secret or "").strip()
            st.rerun()


def render_sidebar_stopwords() -> set[str]:
    st.sidebar.header("ë¶ˆìš©ì–´(Stopwords)")
    base_stop = load_stopwords_file(STOPWORDS_PATH)
    extra_stop = st.sidebar.text_area("ì¶”ê°€ ë¶ˆìš©ì–´(ì¤„ë°”ê¿ˆìœ¼ë¡œ ì…ë ¥)", value="", height=120)
    extra_stop_set = {w.strip() for w in extra_stop.splitlines() if w.strip()}
    stopwords = base_stop | extra_stop_set
    st.sidebar.caption(f"í˜„ì¬ ë¶ˆìš©ì–´ ìˆ˜: {len(stopwords)} (íŒŒì¼ + ì¶”ê°€ ì…ë ¥)")
    return stopwords


def render_main_form():
    with st.form("search", clear_on_submit=False):
        category = st.selectbox("ë¶„ì•¼:", ["ê²½ì œ", "ì •ì¹˜", "ì‚¬íšŒ", "êµ­ì œ", "ì—°ì˜ˆ", "IT", "ë¬¸í™”"])
        user_keyword = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ(í•„ìˆ˜):", value="", placeholder="ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI, ë©”íƒ€ë²„ìŠ¤ ...")
        display = st.select_slider("ë¶„ëŸ‰(ê¸°ì‚¬ ìˆ˜):", options=[100, 200, 300, 400, 500], value=100)
        mask = st.radio("ë°±ë§ˆìŠ¤í¬:", ["ì—†ìŒ", "íƒ€ì›", "ë§í’ì„ ", "í•˜íŠ¸"], horizontal=True)

        r1c1, r1c2, r1c3 = st.columns([1, 1, 1])
        with r1c1:
            show_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë³´ê¸°", value=True)
        with r1c2:
            show_links = st.checkbox("ë§í¬ ì œê³µ", value=False)
        with r1c3:
            dl_articles = st.checkbox("ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)", value=False)

        r2c1, r2c2, r2c3 = st.columns([1, 1, 1])
        with r2c1:
            show_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë³´ê¸°", value=True)
        with r2c2:
            dl_keywords = st.checkbox("í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)", value=False)
        with r2c3:
            dl_images = st.checkbox("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)", value=False)

        submitted = st.form_submit_button("OK")

    return {
        "category": category,
        "user_keyword": user_keyword,
        "display": display,
        "mask": mask,
        "show_articles": show_articles,
        "show_links": show_links,
        "dl_articles": dl_articles,
        "show_keywords": show_keywords,
        "dl_keywords": dl_keywords,
        "dl_images": dl_images,
        "submitted": submitted,
    }


# ===================================================
# 9) ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ===================================================
def run_app():
    st.set_page_config(page_title="ë‰´ìŠ¤ í‚¤ì›Œë“œ ì‹œê°í™”", layout="wide")
    setup_matplotlib_korean_font()
    render_header_with_lottie()

    render_sidebar_api_settings()
    stopwords = render_sidebar_stopwords()

    form = render_main_form()
    if not form["submitted"]:
        return

    if not form["user_keyword"].strip():
        st.warning("ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´, AI)")
        return

    client_id = st.session_state.get("client_id", "").strip()
    client_secret = st.session_state.get("client_secret", "").strip()
    if not client_id or not client_secret:
        st.error("API ì¸ì¦ ì •ë³´(Client ID/Secret)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ Client ID/Secretì„ ì…ë ¥ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return

    final_keyword = build_final_keyword(form["category"], form["user_keyword"])

    st.info(f"ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ ì¤‘... (ê²€ìƒ‰ì–´: {final_keyword})")
    items = fetch_news_items(final_keyword, form["display"], client_id, client_secret)

    if not items:
        st.warning("ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.info("ê°€ëŠ¥í•œ ì›ì¸: (1) ì¸ì¦ ì‹¤íŒ¨ (2) ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (3) ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return

    df_items = build_items_dataframe(items)

    if form["show_articles"]:
        st.subheader("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡")
        if df_items.empty:
            st.warning("ê¸°ì‚¬ ëª©ë¡ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.dataframe(df_items[["title", "pubDate"]], use_container_width=True)

            if form["show_links"]:
                st.caption("ê¸°ì‚¬ ë§í¬(í´ë¦­):")
                for _, r in df_items.head(30).iterrows():
                    if r["link"]:
                        st.markdown(f"- [ğŸ”— ë°”ë¡œê°€ê¸°]({r['link']}) â€” {r['title']}")

    st.info("ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
    docs_clean = collect_corpus_from_items(items)

    if len(docs_clean) < 5:
        st.warning("ë³¸ë¬¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.")
        st.info(
            "ê°œì„  íŒ:\n"
            "- ë¶„ëŸ‰ì„ 200~500ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”.\n"
            "- í‚¤ì›Œë“œë¥¼ ë” ë„“ê²Œ/ì¼ë°˜ì ìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”.\n"
            "- ë§í¬ ì œê³µì„ ì¼œì„œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•´ë³´ì„¸ìš”."
        )
        return

    st.info("í‚¤ì›Œë“œ ë¶„ì„ ì¤‘(ëª…ì‚¬ í•„í„° + TF-IDF)...")
    docs_tokens = tokenize_and_filter_docs(docs_clean, stopwords)

    score_dict = compute_tfidf_scores(docs_tokens, top_k=80)
    if not score_dict:
        st.warning("í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ë°ì´í„°/í•„í„° ì¡°ê±´ ë¶€ì¡±).")
        st.info(
            "ê°œì„  íŒ:\n"
            "- ë¶ˆìš©ì–´ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ë‹¨ì–´ê°€ ê±°ì˜ ë‚¨ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
            "- ë¶„ëŸ‰ì„ ëŠ˜ë¦¬ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ ë°”ê¿” ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
        )
        return

    df_kw, df_kw_top50, df_kw_top20 = build_keyword_tables(score_dict)

    if form["show_keywords"]:
        st.subheader("í‚¤ì›Œë“œ(TF-IDF) ìƒìœ„ 50")
        st.dataframe(df_kw_top50, use_container_width=True)

    st.info("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘...")
    wc_fig = make_wordcloud_figure(score_dict, form["mask"])
    if wc_fig is None:
        st.warning("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨(ë°ì´í„° ë¶€ì¡±).")
        return
    st.pyplot(wc_fig)

    st.info("Top20 ë§‰ëŒ€ì°¨íŠ¸ ìƒì„± ì¤‘...")
    top20_fig = make_top20_bar_figure(df_kw_top20)
    if top20_fig is None:
        st.warning("Top20 ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨(ë°ì´í„° ë¶€ì¡±).")
        return
    st.pyplot(top20_fig)

    st.markdown("---")
    st.subheader("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")

    can_articles = not df_items.empty
    can_keywords = not df_kw_top50.empty
    can_images = (wc_fig is not None) and (top20_fig is not None)

    base = safe_filename(final_keyword)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    c1, c2, c3 = st.columns(3)

    with c1:
        st.download_button(
            label="ê¸°ì‚¬ ëª©ë¡ ë‹¤ìš´ë¡œë“œ(.csv)",
            data=df_items.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig") if can_articles else b"",
            file_name=f"articles_{base}_{ts}.csv",
            mime="text/csv",
            disabled=not (form["dl_articles"] and can_articles),
        )

    with c2:
        st.download_button(
            label="í‚¤ì›Œë“œ í‘œ ë‹¤ìš´ë¡œë“œ(.csv)",
            data=df_kw_top50.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig") if can_keywords else b"",
            file_name=f"keywords_{base}_{ts}.csv",
            mime="text/csv",
            disabled=not (form["dl_keywords"] and can_keywords),
        )

    with c3:
        zip_bytes = make_images_zip_bytes(wc_fig, top20_fig, f"{base}_{ts}") if can_images else b""
        st.download_button(
            label="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ(.png)",
            data=zip_bytes,
            file_name=f"images_{base}_{ts}.zip",
            mime="application/zip",
            disabled=not (form["dl_images"] and can_images),
        )


if __name__ == "__main__":
    run_app()
